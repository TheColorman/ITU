{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Exercises 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Revisit the 10-nearest neighbour classifier that you made in Exercises set 7 based on the\n",
    "dataset `Ex1-training.csv` with two features ($x_1$ and $x_2$) and three classes (Black = 1, Red = 2, Blue = 3).  \n",
    "  \n",
    "In fact, the provided training and test data were balanced – there were equally many instances in each of\n",
    "the three classes – and this distribution of class instances does not reflect that of future datasets to which\n",
    "we wish to apply the classifier. Informed by other studies we know that the prior probabilities of classes\n",
    "would be\n",
    "$$\n",
    "\\mathrm{P}(Y=y)=\\begin{cases}\n",
    "    0.0001, & \\text{if}\\ y=\\text{Black} \\\\\n",
    "    0.02,   & \\text{if}\\ y=\\text{Red} \\\\\n",
    "    0.979,  & \\text{if}\\ y=\\text{Blue}\n",
    "\\end{cases}\n",
    "$$\n",
    "  \n",
    "Modify your 10-nearest-neighbour classifier to reflect this prior class distribution and visualise the resulting\n",
    "decision regions. Describe how they have changed compared to the visualisation you made in Exercise set\n",
    "7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't understand what they want us to do here. How do you modify a KNN with prior probabilities when it uses exclusively posterior probabilities of training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Assuming that you are given an expression for $p(x,y)$, explain how you can achieve the marginal distributions, $p(y)$ and $p(x)$, as well as the conditional distributions $p(y|x)$ and $p(x|y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:  \n",
    "$$\n",
    "p(x)=\\int_{-\\infty}^\\infty p(x,y)\\ \\mathrm{d}y\n",
    "$$\n",
    "$$\n",
    "p(y)=\\int_{-\\infty}^\\infty p(x,y)\\ \\mathrm{d}x\n",
    "$$\n",
    "$$\n",
    "p(x|y)=\\frac{x,y}{y}\n",
    "$$\n",
    "visa versa for y|x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.** Imagine that you wish to detect whether a person has cancer $(Y = 1)$ from some continuous\n",
    "measurement, e.g. a blood marker.  \n",
    "\n",
    "Assume that the class conditionals are normal distributions as\n",
    "$$\n",
    " X|Y=1\\sim \\mathcal{N} (3, 2) \\text{ and } X|Y=2 \\sim \\mathcal{N} (1, 4)\n",
    "$$\n",
    "and that class priors are $P(Y = 1) = 0.2$ and $P(Y = 2) = 0.8$.  \n",
    "\n",
    "1. Explain how to find Bayes classifier and illustrate on a plot both the decision regions and the two\n",
    "functions\n",
    "$$\n",
    "g_k(x) = p (x|Y = k) P(Y = k), k = 1, 2.\n",
    "$$\n",
    "2. Normalise the plot above to show for each value of $x$ the posterior class probabilities $P (Y = k|X = x)$.\n",
    "3. What is the Bayes error rate, i.e. the misclassification error for the Bayes classifier?\n",
    "4. How does the classifier change, if instead $P(Y = 1) = 0.5$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look man idk what to tell you i have no fucking clue what that means it was nowhere in the reading material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**. Consider again the model of Exercise 3. Now we change the loss function from misclassification error to a loss function specified by the following matrix\n",
    "$$\n",
    "L=\\begin{bmatrix}\n",
    "0 & 1000 \\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "1. Explain why you may wish to use a more complex loss function than 0-1 loss.\n",
    "2. Explain how to find a classifier that minimises the expected loss under this new loss function. (*Hint:\n",
    "use the posterior class probabilities from Exercise 3*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution:\n",
    "1. Cause it doesn't take the modified loss function into account from the matrix. We want to penalise false positives significantly more than false negatives.  \n",
    "2. IDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5.** Now, in practice you would not know the Gaussian parameters, nor the class priors, so you\n",
    "need to add an inference step.\n",
    "1. Simulate 1000 observations from the model in Exercise 3 and train the (plug-in) Bayes classifier on\n",
    "these.\n",
    "2. Simulate a test data set of 200 observations and find the confusion matrix for your classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the fuck do you mean simulate \"from the model\"?? The model isn't creating these observations out of nowhere.\n",
    "# These exercises have nothing to do with what was taught."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "malea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
