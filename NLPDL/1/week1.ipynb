{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Second Year Project\n",
    "\n",
    "---\n",
    "\n",
    "Welcome to the weekly assignment of week 1. The assignments are split up per lecture. You can upload your solutions on LearnIt.\n",
    "\n",
    "# Lecture 1. What are words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Regular Expressions\n",
    "For this section, it might be handy to use the website https://regex101.com/ to test your solutions.\n",
    "\n",
    "- a) Write a regular expression (regex or pattern) that matches any of the following words: `cat`, `sat`, `mat`.\n",
    "<br>\n",
    "(Bonus: What is a possible long solution? Can you find a shorter solution? *hint*: match characters instead of words)\n",
    "- b) Write a regular expression that matches numbers, e.g. 12, 1,000, 39.95\n",
    "- c) Expand the previous solution to match Danish price indications, e.g., `1,000 kr` or `39.95 DKK` or `19.95`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "r = r\"[csm]at\"\n",
    "# b)\n",
    "r = r\"(\\d+)(?:[,.](\\d+))*\"\n",
    "# c) \n",
    "r = r\"((\\d+)(?:[,.](\\d+))*)(?:\\s*)(kr|DKK)?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "(Adapted notebook from S. Riedel, UCL & Facebook: https://github.com/uclnlp/stat-nlp-book).\n",
    "\n",
    "In Python, a simple way to tokenize a text is via the `split` method that divides a text wherever a particular substring is found. In the code below this pattern is simply the whitespace character, and this seems like a reasonable starting point for an English tokenization approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan.\\nWhy',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit?']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Mr. Bob Dobolina is thinkin' of a master plan.\" + \\\n",
    "       \"\\nWhy doesn't he quit?\"\n",
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make more fine-grained decisions, we will focus on using regular expressions for tokenization in this assignment. This can be done by either:\n",
    "1. Defining the character sequence patterns at which to split.\n",
    "2. Specifying patters that define what constitutes a token. \n",
    "\n",
    "In the code below we use a simple pattern `\\s` that matches **any whitespace** to define where to split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan.',\n",
       " 'Why',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "gap = re.compile('\\s')\n",
    "gap.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One **shortcoming** of this tokenization is its treatment of punctuation because it considers `plan.` as a token whereas ideally we would prefer `plan` and `.` to be distinct tokens. It might be easier to address this problem if we define what a token is, instead of what constitutes a gap. Below we have defined tokens as sequences of alphanumeric characters and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr',\n",
       " '.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " 'thinkin',\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan',\n",
       " '.',\n",
       " 'Why',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'he',\n",
       " 'quit',\n",
       " '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = re.compile('\\w+|[.?:]')\n",
    "token.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still isn't perfect as `Mr.` is split into two tokens, but it should be a single token. Moreover, we have actually lost an apostrophe. Both are fixed below, although we now fail to break up the contraction `doesn't`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr.',\n",
       " 'Bob',\n",
       " 'Dobolina',\n",
       " 'is',\n",
       " \"thinkin'\",\n",
       " 'of',\n",
       " 'a',\n",
       " 'master',\n",
       " 'plan',\n",
       " '.',\n",
       " 'Why',\n",
       " \"doesn't\",\n",
       " 'he',\n",
       " 'quit',\n",
       " '?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = re.compile('Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we have an input text and apply the tokenizer (described previously) on the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Curiouser\", 'and', 'curiouser', \"'\", 'cried', 'Alice', 'she', 'was', 'so', 'much']\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"\"\"'Curiouser and curiouser!' cried Alice (she was so much surprised, that for the moment she quite\n",
    "forgot how to speak good English); 'now I'm opening out like the largest telescope that ever was! Good-bye,\n",
    "feet!' (for when she looked down at her feet, they seemed to be almost out of sight, they were getting so far\n",
    "off). 'Oh, my poor little feet, I wonder who will put on your shoes and stockings for you now, dears? I'm sure I\n",
    "shan't be able! I shall be a great deal too far off to trouble myself about you: you must manage the best\n",
    "way you can; —but I must be kind to them,' thought Alice, 'or perhaps they won't walk the way I want to go!\n",
    "Let me see: I'll give them a new pair of boots every Christmas...'\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile('Mr.|[\\w\\']+|[.?]')\n",
    "tokens = token.findall(text)\n",
    "print(tokens[:10])\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "* a) The tokenizer clearly makes a few mistakes. Where?\n",
    "\n",
    "* b) Write a tokenizer to tokenize the text correctly by your own definition.\n",
    "\n",
    "* c) Should one separate `'m`, `'ll`, `n't`, possessives, and other forms of contractions from the word? Implement a tokenizer that separates these, and attaches the `'` to the latter part of the contraction.\n",
    "\n",
    "* d) Should elipsis (...) be considered as three `.`s or one `...`? Design a regular expression for both solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "a) Including the first quote, `'`, skipping the exclamation mark, skipping parentheses...  \n",
    "  \n",
    "b)  \n",
    "```re\n",
    "/'|[—\\w-]+|\\.{3}|[[:punct:]]/gm\n",
    "```  \n",
    "  \n",
    "c)  \n",
    "```re\n",
    "/(?<!\\w)'(?=\\w|[[:punct:]])|(?<=\\w|[[:punct:]])'(?!\\w)|'?\\w+|\\.{3}|[!\"#$%&()*+,\\-.\\/:;<=>?@[\\\\\\]^_`{|}~—]/gm\n",
    "```\n",
    "  \n",
    "d) I already did, just remove the `|\\.{3}|` part to capture them as 3 individual periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Twitter Tokenization\n",
    "As you might imagine, tokenizing tweets differs from standard tokenization. There are 'rules' on what specific elements of a tweet might be (mentions, hashtags, links), and how they are tokenized. The goal of this exercise is not to create a bullet-proof Twitter tokenizer but to understand tokenization in a different domain.\n",
    "\n",
    "In the next exercises, we will focus on the following tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"@robv New vids coming tomorrow #excited_as_a_child, can't w8!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['robv', 'New', 'vids', 'coming', 'tomorrow', 'excited_as_a_child', 'can', 't', 'w8']\n"
     ]
    }
   ],
   "source": [
    "token = re.compile('[\\w]+')\n",
    "tokens = token.findall(tweet)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) What is the correct tokenization of the tweet above according to you?\n",
    "- b) Try your tokenizer from the previous exercise (Question 2). Which cases are going wrong? Rewrite your tokenizer such that it handles the above tweet correctly.\n",
    "- c) How will your tokenizer handle emojis?\n",
    "- d) Think of at least one example where your tokenizer (from b) will behave incorrectly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "a) `@robv` `New` `vids` `coming` `tomorrow` `#excited_as_a_child` `,` `can` `'t` `w8` `!` `!`  \n",
    "  \n",
    "b) It does not include the @ and the # in the tokens.  \n",
    "```re\n",
    "/(?<!\\w)'(?=\\w|[[:punct:]])|(?<=\\w|[[:punct:]])'(?!\\w)|['@#]?\\w+|\\.{3}|[!\"#$%&()*+,\\-.\\/:;<=>?@[\\\\\\]^_`{|}~—]/gm\n",
    "```\n",
    "  \n",
    "c) Ignore them.  \n",
    "  \n",
    "d) Emojis. Special characters in usernames (e.g. @hello.world will get tokenized as `@hello` `.` `world`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence segmentation is not a trivial task either.\n",
    "\n",
    "First, make sure you understand the following sentence segmentation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sentence_segment(match_regex, tokens):\n",
    "    \"\"\"\n",
    "    Splits a sequence of tokens into sentences, splitting wherever the given matching regular expression\n",
    "    matches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens      the input sequence as list of strings (each item is a ``word'')\n",
    "    match_regex the regular expression that defines at which token to split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of token lists, where each inner list represents a sentence.\n",
    "\n",
    "    >>> tokens = ['the','man','eats','.','She', 'sleeps', '.']\n",
    "    >>> sentence_segment(re.compile('\\.'), tokens)\n",
    "    [['the', 'man', 'eats', '.'], ['She', 'sleeps', '.']]\n",
    "    \"\"\"\n",
    "    sentences = [[]]\n",
    "    for tok in tokens:\n",
    "        sentences[-1].append(tok)\n",
    "        if match_regex.match(tok):\n",
    "            sentences.append([])\n",
    "            \n",
    "    if sentences[-1] == []:\n",
    "        del sentences[-1]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, there is a variable `text` containing a small text and a regular expression-based segmenter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch', 'is', 'the', 'longest', 'official', 'one', 'word', 'placename', 'in', 'U', '.']\n",
      "['K', '.']\n",
      "[\"Isn't\", 'that', 'weird', '?', 'I', 'mean', 'someone', 'took', 'the', 'effort', 'to', 'really', 'make', 'this', 'name', 'as', 'complicated', 'as', 'possible', 'huh', '?', 'Of', 'course', 'U', '.']\n",
      "['S', '.']\n",
      "['A', '.']\n",
      "['also', 'has', 'its', 'own', 'record', 'in', 'the', 'longest', 'name', 'albeit', 'a', 'bit', 'shorter', '...']\n",
      "['This', 'record', 'belongs', 'to', 'the', 'place', 'called', 'Chargoggagoggmanchauggagoggchaubunagungamaugg', '.']\n",
      "[\"There's\", 'so', 'many', 'wonderful', 'little', 'details', 'one', 'can', 'find', 'out', 'while', 'browsing', 'http', 'www', '.']\n",
      "['wikipedia', '.']\n",
      "['org', 'during', 'their', 'Ph', '.']\n",
      "['D', '.']\n",
      "['or', 'an', 'M', '.']\n",
      "['Sc', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch is the longest official one-word placename in U.K. Isn't that weird? I mean, someone took the effort to really make this name as complicated as possible, huh?! Of course, U.S.A. also has its own record in the longest name, albeit a bit shorter... This record belongs to the place called Chargoggagoggmanchauggagoggchaubunagungamaugg. There's so many wonderful little details one can find out while browsing http://www.wikipedia.org during their Ph.D. or an M.Sc.\n",
    "\"\"\"\n",
    "\n",
    "token = re.compile('Mr.|[\\w\\']+|[.?]+')\n",
    "\n",
    "tokens = token.findall(text)\n",
    "sentences = sentence_segment(re.compile('\\.'), tokens)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- a) Improve the segmenter so that it segments the text in the way you think is correct.\n",
    "- b) How could you deal with all URLs effectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "a)  \n",
    "```re\n",
    "/((?<!\\w)'(?=\\w|[[:punct:]]))|((?<=\\w|[[:punct:]])'(?!\\w))|(\\w+-\\w+)|((?:U\\.S\\.A|Ph\\.D|M\\.Sc|U\\.K)(?:\\.(?=\\s[a-z]))?)|((?:https?:\\/\\/)?\\w+(?:\\.(?:\\w+)?)+(?!\\.)\\w{2,}(?:\\/[A-z0-9]+)*\\/?)|(['@#]?\\w+)|(\\.{3})|([!\"#$%&()*+,\\-.\\/:;<=>?@[\\\\\\]^_`{|}~—])/gm\n",
    "```\n",
    "Then split on matches that only include a period. Breakdown:  \n",
    "`((?<!\\w)'(?=\\w|[[:punct:]]))|((?<=\\w|[[:punct:]])'(?!\\w))`: Match any single quote that has empty space either to the left or right or both e.g. \"'hi' she said\".  \n",
    "`(\\w+-\\w+)`: Match hyphenated words e.g. \"one-word\".  \n",
    "`((?:U\\.S\\.A|Ph\\.D|M\\.Sc|U\\.K)(?:\\.(?=\\s[a-z]))?)`: Match `U.S.A`, `Ph.D`, `M.Sc` and `U.K`. Additionally, include a period if the next characters are a whitespace followed by a lowercase letter.  \n",
    "`((?:https?:\\/\\/)?\\w+(?:\\.(?:\\w+)?)+(?!\\.)\\w{2,}(?:\\/[A-z0-9]+)*\\/?)`: My attempt at matching URLs. A lot of unessecary repeating though.  \n",
    "`(['@#]?\\w+)`: Match hashtags and mentions.  \n",
    "`(\\.{3})`: Match ellipses.  \n",
    "`([!\"#$%&()*+,\\-.\\/:;<=>?@[\\\\\\]^_{|}~—])`: Match any symbol.\n",
    "  \n",
    "b) See above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Tokenization competition\n",
    "\n",
    "Tokenization of social media can be more challenging. We provide a small development set for you to experiment with, which you can find in `week1/tok.dev.txt`. The file is a tab-separated file, where the first column contains the input text, and the second column the gold tokenization (as decided by an annotator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Found the scarriest mystery door in my school. I'M SO CURIOUS D:\", \"Found the scarriest mystery door in my school . I 'M SO CURIOUS D:\"]\n"
     ]
    }
   ],
   "source": [
    "data = [line.strip().split('\\t') for line in open('tok.dev.txt', encoding=\"utf-8\")]\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a test file with the same format, but the gold annotation is missing. This can be found in `week1/tok.test.txt`. You are supposed to develop your tokenizer based on the development data, and then apply your tokenizer on the test data. You can hand in the predictions on the test data on LearnIt in the same slot as the rest of the assignment. We will use F1 score for evaluation.\n",
    "\n",
    "Make sure that the file you hand in:\n",
    "- Uses exactly the same format as the dev data (`input\\<tab\\>output`), where the input and output contain the same characters (except for placement of whitespaces). \n",
    "- Has your ITU username as the name of the file: i.e. `robv.txt`.\n",
    "\n",
    "We have provided an evaluation script for your convenience, it return F1 score, recall, and precision. It also prints out all sentences where your model made an error (indicating the error in red if supported by your terminal), and checks whether your output is in the right format. It can be found in `week1/tok_eval.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2: Language (correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spelling correction\n",
    "\n",
    "Below is an implementation of the Levenshtein distance. It uses a some efficiency tricks, and it is not important you understand every line of this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def levenshteinDistance(s1, s2):\n",
    "    if len(s1) > len(s2):\n",
    "        s1, s2 = s2, s1\n",
    "\n",
    "    distances = range(len(s1) + 1)\n",
    "    for i2, c2 in enumerate(s2):\n",
    "        distances_ = [i2+1]\n",
    "        for i1, c1 in enumerate(s1):\n",
    "            if c1 == c2:\n",
    "                distances_.append(distances[i1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[i1], distances[i1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshteinDistance('this', 'that'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with an English word list from [Aspell](http://aspell.net/) in `aspell-en-dict.txt`. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brower browser\n",
      "False\n",
      "True\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load wordlist (one word per line)\n",
    "en_dict = set([word.strip() for word in open('aspell-en-dict.txt', encoding=\"utf-8\").readlines()])\n",
    "    \n",
    "# Example usage\n",
    "typo = 'brower'\n",
    "correction = 'browser'\n",
    "print(typo, correction)\n",
    "print(typo in en_dict)\n",
    "print(correction in en_dict)\n",
    "print(levenshteinDistance(typo, correction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Implement a (naive) spelling correction system that finds the word in the word list with the smallest minimum edit distance for a word that contains a misspelling. \n",
    "* b) There could be multiple words with the smallest minimum edit distance for some typos, what are supplementary methods to re-rank these? (mention at least 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'browner'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a)\n",
    "def spellcheck(word: str):\n",
    "    if word in en_dict:\n",
    "        return word\n",
    "    \n",
    "    return sorted([(levenshteinDistance(word, w), w) for w in en_dict], key=lambda x: x[0])[0][1]\n",
    "\n",
    "spellcheck(\"brower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) 1. Secondary sort by how common the words are. 2. Factor in key distance for typos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Analysis of spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide you with a list of 100 typos and their corrections from the [GitHub Typo Corpus](https://aclanthology.org/2020.lrec-1.835/) in `typos.txt`. It can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "browers browser\n"
     ]
    }
   ],
   "source": [
    "# Load github typo corpus misspellings\n",
    "typos = []\n",
    "corrections = []\n",
    "for line in open('typos.txt', encoding=\"utf-8\"):\n",
    "    tok = line.strip().split('\\t')\n",
    "    typos.append(tok[0])\n",
    "    corrections.append(tok[1])\n",
    "    \n",
    "# Example usage\n",
    "print(typos[0], corrections[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Evaluate the spelling correction system you implemented in the previous assignment with accuracy. How many of the words did it correct right?\n",
    "* b) Now evaluate the errors, can you identify some common causes (i.e. trends) in the mistakes of your model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# a\n",
    "%pip install numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array(corrections)\n",
    "y_pred = np.array([spellcheck(x) for x in typos])\n",
    "\n",
    "n_correct = sum(y == y_pred)\n",
    "accuracy = n_correct / len(y)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
