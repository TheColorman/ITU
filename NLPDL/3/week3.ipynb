{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - NLP and Deep Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 - Language Identification with a Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this exercise, you will implement the forward step of a FFNN from scratch and compare your solution to Pytorch on a small toy example to predict the language for a given word. \n",
    "\n",
    "It is very important that you understand the basic building blocks (input/output: how to encode your instances, the labels; the model: what the neural network consists of, how to learn its weights, how to do a forward pass for prediction). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Representing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming multi-class classification tasks for the assignments of this week. The labels are: $$ y \\in \\{da,nl,en\\}$$\n",
    "\n",
    "We will use the same data as in week2, from:\n",
    "* English [Wookipedia](https://starwars.fandom.com/wiki/Main_Page)  \n",
    "* Danish [Kraftens Arkiver](https://starwars.fandom.com/da/wiki) \n",
    "* Dutch [Yodapedia](https://starwars.fandom.com/da/wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_langid(path):\n",
    "    text = []\n",
    "    labels = []\n",
    "    for line in open(path, encoding=\"utf-8\"):\n",
    "        tok = line.strip().split('\\t')\n",
    "        labels.append(tok[0])\n",
    "        text.append(tok[1])\n",
    "    return text, labels\n",
    "\n",
    "wooki_train_text, wooki_train_labels = load_langid('langid-data/wookipedia_langid.train.tok.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a): Convert the training data into n-hot format, where each feature represents whether a **single character** is present or not.  Similarly, convert the labels into numeric format. For simplicity, you can assume a closed vocabulary (only the letters in wookie_train_text, no unknown-character handling). Keep original casing, and assign the character indices based on their chronological order.\n",
    "\n",
    "  * What is the vocabulary size?\n",
    "  \n",
    "**Hint:** It is easier for the rest of the assignment if you directly use a torch tensor to save the features ([tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)), a 2d torch tensor filled with 0's can be initiated with: `torch.zeros(dim1, dim2, dtype=float)`. Note the use of `float` instead of `int` here, which is only because the `torch.mm` requires float tensors as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "131\n",
      "['en' 'nl' 'da']\n"
     ]
    }
   ],
   "source": [
    "# a)\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class OneHotCharEncoder:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def fit(self, text: str) -> torch.Tensor:\n",
    "        self.text = text\n",
    "        uniq = pd.unique(np.array(list(text))) # get unique characters in order\n",
    "        self.vocab = uniq\n",
    "\n",
    "        return self.encode(text) # use vocab to create a one-hot vector\n",
    "\n",
    "    def _validate(self):\n",
    "         if self.vocab is None:\n",
    "              raise ValueError(\"Call fit first\")\n",
    "\n",
    "    def encode(self, text: str | list) -> torch.Tensor:\n",
    "            self._validate()\n",
    "            # not that efficient as its not vectorized but the vocab is pretty small so shouldn't be a big deal\n",
    "            return torch.tensor([1 if c in text else 0 for c in self.vocab], dtype=float)\n",
    "    def transform(self, text: list) -> torch.Tensor:\n",
    "         return torch.stack([self.encode(t) for t in text])\n",
    "\n",
    "enc = OneHotCharEncoder()\n",
    "enc.fit(\"\".join(wooki_train_text))\n",
    "wooki_train_tensors = enc.transform(wooki_train_text)\n",
    "print(wooki_train_tensors) # each row is a datapoint\n",
    "print(len(enc.vocab)) # 131 chars\n",
    "\n",
    "labels = pd.unique(np.array(wooki_train_labels))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2: Forward pass (from scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Networks (FNNs) or MLPs\n",
    "\n",
    "Feedforward Neural Networks (FNNs) are also called Multilayer Perceptrons (MLPs). These are the most basic types of neural networks. They are called this way as the information is flowing from the input nodes through the network up to the output nodes. \n",
    "\n",
    "It is essential to understand that a neural network is a non-linear classification model which is based upon function application. Each layer in a neural network is an application of a function.\n",
    "\n",
    "Summary (by J.Frellsen):\n",
    "<img src=\"pics/fnn_jf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to implement the forward step manually on a small dataset. You will create a network following the design in the following figure (note that the input should be the sames size as the number of characters found in the previous assignment, instead of 4):\n",
    "\n",
    "<img src=\"pics/nn.svg\">\n",
    "\n",
    "a) How many neurons do hidden layer 1 and hidden layer 2 have? Note: the bias node is not shown in the figure, you do not have to count them for this assignment.\n",
    "\n",
    "b) How many neurons does the output layer have? And the input layer? (Note: the figure shows only 4 input nodes, in this example your input size is defined in the previous assignment - what is the input layer size?)\n",
    "\n",
    "c) Specify the size of layers of the feedforward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions to determine the input and output dimensions of each layer\n",
    "input_dim = len(enc.vocab)\n",
    "\n",
    "hidden_dim1 = 15\n",
    "hidden_dim2 = 20\n",
    "\n",
    "output_dim = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Now initialize the layers themselves as torch tensors (do not use a torch.module here!). You can define the bias and the weights in separate tensors. The weights should be initialized randomly (`torch.randn((dim1, dim2), dtype=torch.float)`, see also [torch.randn](https://pytorch.org/docs/stable/generated/torch.randn.html)) and the biases can be set to 1 (`torch.ones(dim1, dtype=torch.float)`, see also [torch.ones](https://pytorch.org/docs/stable/generated/torch.ones.html)). Confirm whether their size match the answer to `b)` and `a)` by printing .shape of the tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define all parameters of this NN\n",
    "\n",
    "# Each row is a neuron, and each column is a datapoint that gets passed to |row| neurons.\n",
    "w1 = torch.randn((input_dim, hidden_dim1), dtype=float)\n",
    "b1 = torch.ones(hidden_dim1, dtype=float)\n",
    "\n",
    "w2 = torch.randn((hidden_dim1, hidden_dim2), dtype=float)\n",
    "b2 = torch.ones(hidden_dim2, dtype=float)\n",
    "\n",
    "w3 = torch.randn((hidden_dim2, output_dim), dtype=float)\n",
    "b3 = torch.randn(output_dim, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the shape of all parameters, we are ready to \"connect the dots\" and build the network. \n",
    "\n",
    "It is instructive to break the computation of each layer down into two steps: the scores $a1$ are obtained by the linear function followed by the activation applications $\\sigma$ to obtain the representation $z1$, as in:\n",
    "\n",
    "$$ a1 = xW_1 + b_1$$\n",
    "$$ z1 = \\sigma(a1)$$\n",
    "\n",
    "d) Specify the entire network up to the output layer $z3$, and **up to and exclusive** the final application of the softmax, the last activation function, which is provided. For multiplication [torch.mm](https://pytorch.org/docs/stable/generated/torch.mm.html) can be used. Use a tanh activation function: [torch.tanh](https://pytorch.org/docs/stable/generated/torch.tanh.html).\n",
    "\n",
    "The exact implementation of the softmax might differ from toolkit to toolkit (due to variations in implementation details in order to obtain numerical stability). Therefore, we will use the Pytorch implementation for the softmax calculation ([torch.nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement the forward pass (up to and exclusive the softmax) \n",
    "## apply it to the training data `data_train` - use vectorization\n",
    "\n",
    "def pred(x: torch.Tensor):\n",
    "    a1 = torch.matmul(x, w1) + b1\n",
    "    z1 = torch.tanh(a1)\n",
    "\n",
    "    a2 = torch.matmul(z1, w2) + b2\n",
    "    z2 = torch.tanh(a2)\n",
    "\n",
    "    a3 = torch.matmul(z2, w3) + b3\n",
    "    return a3\n",
    "\n",
    "wooki_preds_a3 = pred(wooki_train_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that all predictions sum up to approximately 1 (hint: use `torch.sum` with `axis=1`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1223,  4.1720,  0.2384,  ...,  0.3482,  1.6423,  5.1073],\n",
      "       dtype=torch.float64)\n",
      "tensor([1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(wooki_preds_a3, axis=1)) # no shit they're not gonna sum to 1 you told me not to do the last softmax\n",
    "m = torch.nn.Softmax(dim=1)\n",
    "wooki_preds = m(wooki_preds_a3)\n",
    "print(torch.sum(wooki_preds, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Congrats! you have made it through the manual construction of the forward pass. Note that these weights are still random, so performance is not expected to be good. Now lets compare your implementation to a set of pre-determined weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Where do the weights come from?  Loading existing weights\n",
    "\n",
    "So far, the model that you used randomly initialized weights. In this step we will load pre-trained model weights and do the forward pass with those weights, in order to check your implementation against model predictions computed by the toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to:\n",
    "* load pretrained weights for all parameters\n",
    "* apply the weights to the evaluation data\n",
    "* check that your manual softmax scores match the ones obtained by the pre-trained model `model` that we will load\n",
    "* convert the output to labels and calculate the accuracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load the pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# use the character indexing from assignment 3\n",
    "idx2char = ['H', 'e', ' ', 'v', 'n', 'w', 't', 's', 'o', 'f', 'a', 'r', 'u', 'g', 'h', ',', 'i', 'c', 'y', 'd', 'b', 'm', 'p', 'l', 'k', '.', 'D', 'E', 'C', 'j', 'R', 'S', 'U', '1', \"'\", 'æ', 'å', 'q', '`', 'I', '(', ')', 'M', 'F', '-', 'x', 'K', '9', '5', 'B', 'W', 'z', 'G', 'P', 'L', '/', 'O', '6', 'T', '7', 'Z', '2', '0', 'J', 'V', 'A', 'ø', 'X', '–', 'N', 'ë', ':', '&', '3', 'Y', 'é', '4', '[', ']', '’', ';', '8', 'É', 'Æ', 'Q', '!', '—', 'ï', '°', 'ō', '\\u200b', '‘', 'ń', '“', '”', '?', 'Å', '<', '>', '#', '%', '+', 'ʊ', 'ɹ', 'ə', 'ɑ', 'ö', 'à', 'á', 'è', '=', 'ü', 'Ø', '∑', '^', 'ś', 'ñ', '|', '½', '$', '«', '™', 'ó', '´', '…', '―', '»', 'ː', 'θ', '²', 'Θ']\n",
    "char2idx = {'H': 0, 'e': 1, ' ': 2, 'v': 3, 'n': 4, 'w': 5, 't': 6, 's': 7, 'o': 8, 'f': 9, 'a': 10, 'r': 11, 'u': 12, 'g': 13, 'h': 14, ',': 15, 'i': 16, 'c': 17, 'y': 18, 'd': 19, 'b': 20, 'm': 21, 'p': 22, 'l': 23, 'k': 24, '.': 25, 'D': 26, 'E': 27, 'C': 28, 'j': 29, 'R': 30, 'S': 31, 'U': 32, '1': 33, \"'\": 34, 'æ': 35, 'å': 36, 'q': 37, '`': 38, 'I': 39, '(': 40, ')': 41, 'M': 42, 'F': 43, '-': 44, 'x': 45, 'K': 46, '9': 47, '5': 48, 'B': 49, 'W': 50, 'z': 51, 'G': 52, 'P': 53, 'L': 54, '/': 55, 'O': 56, '6': 57, 'T': 58, '7': 59, 'Z': 60, '2': 61, '0': 62, 'J': 63, 'V': 64, 'A': 65, 'ø': 66, 'X': 67, '–': 68, 'N': 69, 'ë': 70, ':': 71, '&': 72, '3': 73, 'Y': 74, 'é': 75, '4': 76, '[': 77, ']': 78, '’': 79, ';': 80, '8': 81, 'É': 82, 'Æ': 83, 'Q': 84, '!': 85, '—': 86, 'ï': 87, '°': 88, 'ō': 89, '\\u200b': 90, '‘': 91, 'ń': 92, '“': 93, '”': 94, '?': 95, 'Å': 96, '<': 97, '>': 98, '#': 99, '%': 100, '+': 101, 'ʊ': 102, 'ɹ': 103, 'ə': 104, 'ɑ': 105, 'ö': 106, 'à': 107, 'á': 108, 'è': 109, '=': 110, 'ü': 111, 'Ø': 112, '∑': 113, '^': 114, 'ś': 115, 'ñ': 116, '|': 117, '½': 118, '$': 119, '«': 120, '™': 121, 'ó': 122, '´': 123, '…': 124, '―': 125, '»': 126, 'ː': 127, 'θ': 128, '²': 129, 'Θ': 130}\n",
    "\n",
    "# the label indexes that were used during training\n",
    "label2idx = {'da':0, 'nl':1, 'en':2}\n",
    "idx2label = ['da', 'nl', 'en']\n",
    "\n",
    "# This is the definition of an FNN model in PyTorch, and can mostly be ignored for now.\n",
    "# We will focus on how to create Torch models in week 5\n",
    "class LangId(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.input = nn.Linear(vocab_size, 15)\n",
    "        self.hidden1 = nn.Linear(15, 20)\n",
    "        self.hidden2 = nn.Linear(20, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.input(x))\n",
    "        x = torch.tanh(self.hidden1(x))\n",
    "        x = self.hidden2(x)\n",
    "        return x\n",
    "\n",
    "lang_classifier = torch.load('model.th')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the weights you just loaded using the `state_dict()` function of the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('input.weight',\n",
       "              tensor([[ 0.1274,  0.2723,  0.4691,  ...,  0.0754,  0.0201,  0.0813],\n",
       "                      [-0.1876,  0.3465,  0.4979,  ..., -0.0436, -0.0362, -0.0866],\n",
       "                      [ 0.1779,  0.3311,  0.3578,  ..., -0.0705,  0.0656,  0.0415],\n",
       "                      ...,\n",
       "                      [-0.0264,  0.2019,  0.1753,  ...,  0.0335,  0.0764,  0.0222],\n",
       "                      [-0.0810, -0.3535, -0.1255,  ..., -0.0645,  0.0299,  0.0438],\n",
       "                      [ 0.0740, -0.1535,  0.1290,  ..., -0.0464, -0.0612,  0.0650]])),\n",
       "             ('input.bias',\n",
       "              tensor([ 0.4091,  0.8057,  0.4696,  0.3282,  0.4459, -0.3094, -0.7575, -0.3531,\n",
       "                      -0.3175,  0.2946,  0.7420,  0.1358,  0.1037, -0.2193, -0.3283])),\n",
       "             ('hidden1.weight',\n",
       "              tensor([[ 0.2575,  0.6953,  0.5631,  0.2704, -0.3716, -0.9438, -0.4709, -0.9932,\n",
       "                       -0.7564, -0.0925, -2.2822,  0.2297,  0.2956,  0.0241, -1.9843],\n",
       "                      [ 0.1413,  0.2127,  0.4845, -0.0701,  0.6703, -0.3185, -0.3957,  0.1315,\n",
       "                       -0.6918, -0.2745, -0.1002,  0.2550,  0.4770,  0.1109, -0.3992],\n",
       "                      [ 0.7823,  0.1207,  0.3213, -0.0475,  0.7223,  0.1428, -0.5531,  0.2000,\n",
       "                        0.0893, -0.3841,  0.1112, -0.0569,  0.5947, -0.6937,  0.4243],\n",
       "                      [ 0.0985,  1.5386, -0.3472, -0.8660,  0.7426, -0.0289, -0.0979, -0.2248,\n",
       "                       -0.0257,  0.4956, -1.0115, -0.2432, -0.4809,  0.6184,  0.6953],\n",
       "                      [ 0.0232, -0.2676, -0.0068,  0.3410, -0.4943,  0.6073,  0.1574,  0.6130,\n",
       "                        0.8132, -0.0182,  1.0047, -0.2493, -0.1170, -0.4713,  0.8951],\n",
       "                      [-0.4531,  0.0077,  0.1982, -0.2570,  0.9576,  1.5319,  1.0911, -0.1294,\n",
       "                       -0.1513,  0.4058,  0.8775, -1.4542, -0.6756,  0.6437, -0.0637],\n",
       "                      [-0.0188,  0.1584, -0.2749,  0.1247,  0.4504,  0.6595,  0.4859, -0.0764,\n",
       "                        0.3448,  0.2107,  0.3694, -0.5843, -0.0219,  0.1169,  0.4314],\n",
       "                      [ 0.2663,  0.4091,  0.4482, -0.0810,  0.6472, -0.0926, -0.1696, -0.2151,\n",
       "                       -0.6046, -0.2196, -0.0106,  0.0498,  0.3613,  0.1652, -0.0080],\n",
       "                      [ 0.0238, -0.7592,  0.3622,  0.2082, -0.1043,  0.3239, -0.0603,  0.2656,\n",
       "                        0.3006, -0.2938,  0.5831, -0.0507, -0.0260, -0.3097,  0.2777],\n",
       "                      [ 0.2607,  0.9150, -0.2169, -0.1303,  0.5222, -0.0239, -0.6132, -0.1086,\n",
       "                       -0.1612,  0.0567,  0.0154,  0.2324, -0.5132,  0.8815,  0.2304],\n",
       "                      [ 0.0487, -0.4816, -0.1768, -0.0849, -0.2191, -0.1497, -0.2482,  0.3167,\n",
       "                        1.2500, -0.0845,  0.8245,  0.4551,  0.0036, -0.5018,  0.0255],\n",
       "                      [-0.0802,  0.5033, -0.0674,  0.0871,  0.2249,  0.4973,  0.3259,  0.4636,\n",
       "                        0.7863, -0.0290,  1.0510, -0.2961, -0.1969,  0.5019,  1.9955],\n",
       "                      [ 0.0207,  0.1539, -0.0628,  0.0138,  0.8202,  0.4971,  0.8012,  0.0564,\n",
       "                       -0.1323, -0.0718, -0.2645, -1.0286, -0.1922,  0.3575,  0.3256],\n",
       "                      [ 0.6240,  0.3164, -0.1598, -0.6158,  0.5465,  0.0340,  0.0693,  0.2907,\n",
       "                        0.7922,  0.1091, -0.0147, -0.6633,  0.0804, -0.1143,  0.5061],\n",
       "                      [ 0.1215,  0.4285,  0.2948, -0.9482,  0.0128, -0.8172, -0.5179, -0.7863,\n",
       "                       -0.8570,  0.0745, -0.5490,  0.6721,  0.2755, -0.2263, -0.3991],\n",
       "                      [-1.0201,  0.0455, -0.9147, -0.7720, -0.4775,  0.5190,  0.5954,  0.3022,\n",
       "                        0.0520,  0.4095,  0.3205, -0.5907, -0.5462, -0.1718,  1.2266],\n",
       "                      [-0.1344, -0.1653, -0.2833, -0.3142, -0.1152,  0.2869,  0.2829,  0.0184,\n",
       "                        0.1363,  0.3274, -0.0947, -0.3415, -0.1789,  0.0378,  0.0306],\n",
       "                      [-0.1025,  0.6285, -0.0914, -0.1482,  0.0396, -0.4274,  0.1675, -0.2574,\n",
       "                       -0.0724, -0.0152,  0.2160, -0.1630,  0.1252,  0.1318,  0.1642],\n",
       "                      [ 0.5360,  0.7464,  0.3229, -0.2490,  0.6192, -0.1805, -0.1130,  0.2933,\n",
       "                       -0.0098,  0.0768,  1.5201, -0.4075,  0.0207,  0.5212,  1.6912],\n",
       "                      [ 0.5786, -0.6360,  0.2668,  0.7761, -0.6159, -1.0443, -0.5363, -0.5601,\n",
       "                       -0.9773,  0.1997, -2.5120,  0.8936,  0.7410,  0.4194, -1.9675]])),\n",
       "             ('hidden1.bias',\n",
       "              tensor([ 0.6749,  0.1967,  0.3827, -0.8974, -0.0010, -0.6915,  0.1325,  0.2587,\n",
       "                      -0.1166, -0.4284,  0.2775,  0.1698, -0.1733, -0.5159, -0.1306, -0.6334,\n",
       "                      -0.0710, -0.0774,  0.4660,  0.7498])),\n",
       "             ('hidden2.weight',\n",
       "              tensor([[-1.1929,  0.7590, -0.4203,  0.6335, -0.5118, -0.6470,  0.1136,  0.7656,\n",
       "                       -0.2115,  0.4258, -0.1070,  1.3044, -0.3927,  0.4815, -0.2065,  0.1193,\n",
       "                        0.5277, -0.0628,  1.0586, -1.2823],\n",
       "                      [-0.1084, -0.3936,  0.2931, -0.8568,  0.0422,  0.2233, -0.1856, -0.4776,\n",
       "                        0.3987, -0.6149,  0.4459, -0.1198,  0.2158, -0.1368, -0.3301, -0.5805,\n",
       "                        0.1345, -0.3114, -0.2366,  0.5852],\n",
       "                      [ 1.2269, -0.5441, -0.0556,  0.6234,  0.3867,  0.3451,  0.2686, -0.6395,\n",
       "                       -0.5229,  0.2870, -0.3788, -1.0749,  0.3720, -0.4522,  0.4057,  0.1022,\n",
       "                       -0.1416, -0.1017, -0.5362,  0.9197]])),\n",
       "             ('hidden2.bias', tensor([ 0.6436,  0.2206, -0.8653]))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_classifier.state_dict()\n",
    "# those sure are weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) Convert the following dev data into the input format for the neural network above. \n",
    "\n",
    "**Hint** The indices of the characters are based on the order in the training data, and should match in the development data, we provide the correct idx2char and char2idx that were used to train the model in the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "wooki_dev_text, wooki_dev_labels = load_langid('langid-data/wookipedia_langid.dev.tok.txt')\n",
    "enc = OneHotCharEncoder()\n",
    "enc.vocab = idx2char # not taking chances that the order is messed up\n",
    "wooki_dev_tensors = enc.transform(wooki_dev_text)\n",
    "print(wooki_dev_tensors) # those sure are numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) run a forward pass on the dev-data with `lang_classifier`, using the forward() function\n",
    "\n",
    "* c) Apply your manual implementation of the forward pass to the evaluation data by using the parameters (weights) you just loaded with `state_dict()`. This allows you to check if you get the same results back as the model implemented in Torch. If the outputs match, you implemented the forward pass correctly, congratulations!\n",
    "\n",
    "**Hint**: internally the torch model saves the weight in a transposed vector for efficiency reasons. This means that W1 will have the dimension of (15,131). To use your previous implementation you have to call the the transpose function in Pytorch ([`.t()`](https://pytorch.org/docs/stable/generated/torch.t.html)), which will convert the shape to be (131,15)\n",
    "\n",
    "* d) Now apply softmax on the resulting weights and convert the output to the label predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "wooki_dev_tensors = wooki_dev_tensors.type(torch.float) # oops\n",
    "torch_result = lang_classifier.forward(wooki_dev_tensors)\n",
    "\n",
    "# starting point for c)\n",
    "w = lambda x: lang_classifier.state_dict()[x].t()\n",
    "w1 = w('input.weight')\n",
    "b1 = w('input.bias')\n",
    "w2 = w('hidden1.weight')\n",
    "b2 = w('hidden1.bias')\n",
    "w3 = w('hidden2.weight')\n",
    "b3 = w('hidden2.bias')\n",
    "my_result = pred(wooki_dev_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        ...,\n",
       "        [True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_result == my_result\n",
    "# wow that is actually pretty unexpected, first try too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.37%\n"
     ]
    }
   ],
   "source": [
    "# d)\n",
    "m = nn.Softmax(dim=1)\n",
    "my_prob = m(my_result)\n",
    "my_pred_idx = torch.argmax(my_prob, axis=1)\n",
    "my_pred = [idx2label[idx] for idx in my_pred_idx]\n",
    "\n",
    "n_correct = np.sum(np.array(my_pred) == np.array(wooki_dev_labels))\n",
    "acc = n_correct / len(my_pred)\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: What do word embeddings represent?\n",
    "In the following exercises, you are going to explore what is represented with word embeddings. You are going to make use of the python gensim package and two sets of pre-trained embeddings. The embeddings can be downloaded from:\n",
    "\n",
    "* http://itu.dk/people/robv/data/embeds/twitter.bin.tar.gz\n",
    "* http://itu.dk/people/robv/data/embeds/GoogleNews-50k.bin.tar.gz\n",
    "\n",
    "The first embeddings are skip-gram embeddings trained on a collection of 2 billion words from English tweets collected during 2012 and 2018 with the default settings of word2vec. The second embeddings are trained on 100 billion words from Google News. They have both been truncated to the most frequent 500,000 words. Note that loading that each of these embeddings require approximately 2GB of ram.\n",
    "\n",
    "The embeddings can be loaded in gensim as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading finished\n"
     ]
    }
   ],
   "source": [
    "import gensim.models\n",
    "\n",
    "twitEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'embeddings/twitter.bin', binary=True\n",
    ")\n",
    "googleEmbs = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'embeddings/GoogleNews-50k.bin', binary=True\n",
    ")\n",
    "\n",
    "print('loading finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use the index operator ``[]`` or the function ``get_vector()`` to acces the individual word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.64285821e-01,  2.37979457e-01, -4.24226150e-02, -4.35831666e-01,\n",
       "       -4.06450212e-01, -1.43117514e-02,  1.22334510e-01, -5.59092343e-01,\n",
       "        1.23332568e-01,  2.36625358e-01,  3.58797014e-02, -9.40739065e-02,\n",
       "       -2.04128489e-01, -1.81295779e-02, -1.08792759e-01, -2.70818472e-01,\n",
       "        1.05479717e-01,  1.37095019e-01,  1.79271579e-01,  2.91243941e-01,\n",
       "       -5.87746739e-01,  2.90462654e-02,  6.89281642e-01, -1.80917114e-01,\n",
       "       -2.57750720e-01, -2.01395631e-01, -5.16403615e-01,  5.85804135e-03,\n",
       "       -1.67768478e-01,  2.17095211e-01,  2.22494245e-01,  1.56742647e-01,\n",
       "       -3.60864878e-01,  3.94283593e-01,  8.04448500e-03,  1.11518592e-01,\n",
       "       -1.85592070e-01, -1.16088443e-01,  3.24357510e-01,  4.00876179e-02,\n",
       "        9.14092362e-02, -1.04118213e-01, -6.89513862e-01,  1.54412836e-01,\n",
       "        4.57625002e-01,  2.55037360e-02, -3.84058757e-03,  7.12698698e-02,\n",
       "       -2.25590184e-01, -1.96693689e-01, -3.88458431e-01, -2.27625713e-01,\n",
       "        6.94357634e-01, -3.22451681e-01,  1.02136515e-01, -2.06018016e-01,\n",
       "        4.12042558e-01, -5.69718063e-01, -1.77221447e-01, -7.04838037e-01,\n",
       "        5.86289287e-01,  1.18259907e-01, -5.15342169e-02,  3.12465429e-01,\n",
       "       -5.25288224e-01,  5.48078716e-01,  2.75395304e-01, -1.61753371e-01,\n",
       "        4.37383980e-01, -9.72139016e-02, -1.71533942e-01,  3.94486845e-01,\n",
       "        1.33596465e-01,  3.94779667e-02,  1.23597078e-01,  3.22522134e-01,\n",
       "       -1.40469015e-01, -7.82357603e-02, -3.39861751e-01, -4.84348953e-01,\n",
       "        8.03721175e-02,  1.13537483e-01, -6.08491674e-02,  2.59142101e-01,\n",
       "        3.79286081e-01, -3.21717769e-01,  3.45237699e-04,  4.53131020e-01,\n",
       "        2.97795296e-01,  4.74226564e-01, -4.53676343e-01,  3.24836336e-02,\n",
       "       -3.32390517e-01, -2.07979798e-01, -1.37789533e-01, -1.56768903e-01,\n",
       "       -4.80998158e-02,  1.80168718e-01,  6.78501278e-03, -9.98419821e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitEmbs['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word similarities\n",
    "Cosine distance can be used to measure the distance between two words. It is defined as:\n",
    "\\begin{equation}\n",
    "cos_{\\vec{a},\\vec{b}} = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}| |\\vec{b}|} = \\frac{\\sum^n_1 a_i b_i}{\\sqrt{\\sum^n_1 a_i^2} \\sqrt{\\sum^n_1 b_i^2}}\n",
    "\\end{equation}\n",
    "\n",
    "* a) Implement the cosine similarity using pure python (only the ``math`` package is allowed). Note that `similarity == 1-distance`.\n",
    "\n",
    "You can compare your scores to the gensim implementation to check wheter it is correct. The following code should give the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10446518659591675\n",
      "0.8955349\n",
      "0.8955349\n"
     ]
    }
   ],
   "source": [
    "def cos(a: np.ndarray, b: np.ndarray):\n",
    "    # I *know* the vectors are already numpy arrays,\n",
    "    # so is using the numpy dot product instead of a loop really cheating?\n",
    "    numerator = a @ b\n",
    "    len_a = np.sqrt(a @ a) # this function is also in math so definitely not cheating\n",
    "    len_b = np.sqrt(b @ b)\n",
    "    denominator = len_a * len_b\n",
    "    return numerator / denominator\n",
    "\n",
    "print(twitEmbs.distance('cat', 'dog'))\n",
    "print(cos(twitEmbs['cat'], twitEmbs['dog']))\n",
    "cat = np.copy(twitEmbs['cat'])\n",
    "dog = np.copy(twitEmbs['dog'])\n",
    "print(np.dot(cat, dog)/(np.linalg.norm(cat)*np.linalg.norm(dog))) # yay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In wordnet, the distance between two senses can be based on the distance in the taxonomy. The most common metric for this is:\n",
    "\n",
    "* Wu-Palmer Similarity: denotes how similar two word senses are, based on the depth of the two senses in the taxonomy and of their Least Common Subsumer (most specific ancestor node).\n",
    "\n",
    "It can be obtained in python like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\thore\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordNet similarity: 0.8571428571428571\n",
      "Twitter similarity: 0.8955348\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "first_word = wordnet.synsets('cat')[0] #0 means: most common sense\n",
    "second_word = wordnet.synsets('dog')[0]\n",
    "print('WordNet similarity: ' + str(first_word.wup_similarity(second_word)))\n",
    "\n",
    "print('Twitter similarity: ' + str(twitEmbs.similarity('cat', 'dog')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* b) Think of 5 word pairs which have a high similarity according to you. Estimate the difference between these pairs in wordnet as well as in the Twitter embeddings and the Google News embeddings. Which method is closest to your own intuition? (You are allowed to use the gensim implementation of cosine similarity here.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mouse - keyboard\n",
      "Twitter: 0.76052743\n",
      "Google:  0.47375855\n",
      "Wordnet: 0.38095238095238093\n",
      "mouse - hamster\n",
      "Twitter: 0.7560636\n",
      "Google:  0.48806348\n",
      "Wordnet: 0.9230769230769231\n",
      "door - window\n",
      "Twitter: 0.8368629\n",
      "Google:  0.62127966\n",
      "Wordnet: 0.631578947368421\n",
      "hand - foot\n",
      "Twitter: 0.7235881\n",
      "Google:  0.22718483\n",
      "Wordnet: 0.8235294117647058\n",
      "television - monitor\n",
      "Twitter: 0.38444385\n",
      "Google:  0.111713566\n",
      "Wordnet: 0.38095238095238093\n",
      "\n",
      "mouse - keyboard:\n",
      "  Twitter has a much hight similarity that the two other ones. That actually does make sense, as you'd expect twitter users to talk more about computers than the average newspaper.\n",
      "mouse - keyboard:\n",
      "  Twitter is very similar. So is Google News, guess there arent that many news stories about hamsters, and mice are probably dominated by lab experiments. Wordnet is pretty high, unsurprisingly.\n",
      "door - window:\n",
      "  ¯\\_(ツ)_/¯\n",
      "television - monitor:\n",
      "  I think it would have been a lot higher if I had used TV. You know what lets try.\n",
      "\n",
      "(0.42712036, 0.13722876, 0.38095238095238093)\n",
      "A little higher, yeah\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "pairs = [\n",
    "    ('mouse', 'keyboard'), # 70%?\n",
    "    ('mouse', 'hamster'), #  80%?\n",
    "    ('door', 'window'), #   75%?\n",
    "    ('hand', 'foot'), #     85%?\n",
    "    ('television', 'monitor'), #  80%?\n",
    "]\n",
    "def get_sims(w1, w2):\n",
    "    twit = twitEmbs.similarity(w1, w2)\n",
    "    goog = googleEmbs.similarity(w1, w2)\n",
    "    wdnt = wordnet.synsets(w1)[0].wup_similarity(wordnet.synsets(w2)[0])\n",
    "    return twit, goog, wdnt\n",
    "\n",
    "for w1, w2 in pairs:\n",
    "    t, g, w = get_sims(w1, w2)\n",
    "    print(w1, \"-\", w2)\n",
    "    print(\"Twitter:\", t)\n",
    "    print(\"Google: \", g)\n",
    "    print(\"Wordnet:\", w)\n",
    "\n",
    "print(\"\"\"\n",
    "mouse - keyboard:\n",
    "  Twitter has a much hight similarity that the two other ones. That actually does make sense, as you'd expect twitter users to talk more about computers than the average newspaper.\n",
    "mouse - keyboard:\n",
    "  Twitter is very similar. So is Google News, guess there arent that many news stories about hamsters, and mice are probably dominated by lab experiments. Wordnet is pretty high, unsurprisingly.\n",
    "door - window:\n",
    "  ¯\\_(ツ)_/¯\n",
    "television - monitor:\n",
    "  I think it would have been a lot higher if I had used TV. You know what lets try.\n",
    "\"\"\")\n",
    "print(get_sims('TV', 'monitor'))\n",
    "print(\"\"\"A little higher, yeah\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analogies\n",
    "\n",
    "Analogies have often been used to demonstrate the power of word embeddings. Analogies have the form ``A :: B : C :: D``. In this setting `A`, `B` and `C` are usually given and the fourth term `D` is extracted from the embeddings by using ``3cosadd``:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{d}{\\mathrm{argmax}} (\\cos (d, c) - \\cos (d, a) + \\cos (d, b))\n",
    "\\end{equation}\n",
    "\n",
    "You can query analogies with gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.8401797413825989),\n",
       " ('goddess', 0.7309160232543945),\n",
       " ('king…', 0.7233694195747375),\n",
       " ('princess', 0.7157886624336243),\n",
       " ('kings', 0.707615852355957),\n",
       " ('godess', 0.6952610015869141),\n",
       " ('Queen', 0.6902579069137573),\n",
       " ('queen,', 0.687620997428894),\n",
       " ('quee…', 0.6856901049613953),\n",
       " ('queens', 0.6832401752471924)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Man is to king as woman is to ...?\n",
    "twitEmbs.most_similar(positive=['woman', 'king'], negative=['man'], \n",
    "                                                         topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``3cosadd`` can be used to solve semantic as well as syntactic analogies:\n",
    "\n",
    "| Semantic            |                                      |\n",
    "|---------------------|--------------------------------------|\n",
    "| Country-capital     | Denmark :: Copenhagen : England :: X |\n",
    "| Family-relations    | boy :: girl : he :: X                |\n",
    "| Object-color        | sky :: blue : grass :: X             |\n",
    "\n",
    "| Syntactic           |                                      |\n",
    "|---------------------|--------------------------------------|\n",
    "| Superlatives        | nice :: nicer : good :: X            |\n",
    "| Present-past tense  | work :: worked : drink :: X          |\n",
    "| Country-nationality | Brazil :: Brazilian : Denmark :: X   |\n",
    "\n",
    "\n",
    "Try the analogies from the table. Is the correct answer returned for all queries? \n",
    "If not: are the answers at least ranked high?\n",
    "\n",
    "* a) Think of another category of *semantic* analogies that might be encoded in the embeddings and test this empirically by thinking of 5 example analogies. Which embeddings are better at predicting your category (Twitter versus Google News)?\n",
    "\n",
    "* b) Think of another category of *syntactic* analogies that might be encoded in the embeddings and test this empirically by thinking of 5 example analogies. Which embeddings are better at predicting your category (Twitter versus Google News)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dublin', 0.791156530380249), ('London', 0.7881399989128113), ('Glasgow', 0.779718816280365), ('Edinburgh', 0.7671084403991699), ('Antwerp', 0.75947505235672)]\n",
      "[('she', 0.9052496552467346), ('she…', 0.782346785068512), ('he/she', 0.7575312256813049), ('it.She', 0.7428714036941528), ('*he*', 0.7421621680259705)]\n",
      "[('mustard', 0.7218776345252991), ('green', 0.719383180141449), ('brown', 0.7046061754226685), ('greens', 0.7032448649406433), ('alfalfa', 0.6829589009284973)]\n",
      "[('better', 0.7841683626174927), ('worse', 0.7769781351089478), ('shittier', 0.7208447456359863), ('better,', 0.7019132971763611), ('crappier', 0.7017417550086975)]\n",
      "[('drank', 0.8187708854675293), ('shotgunned', 0.7269829511642456), ('chugged', 0.7176188826560974), ('drinked', 0.7070199251174927), ('sipped', 0.7053581476211548)]\n",
      "[('Dutch', 0.6439822912216187), ('Moroccan', 0.6369330883026123), ('Tanned', 0.6136460304260254), ('Czech', 0.6110645532608032), ('brazillian', 0.6096874475479126)]\n",
      "\n",
      "[('hedgehog', 0.7189444303512573), ('cow', 0.7182788848876953), ('tiger', 0.710384726524353), ('lemur', 0.705183207988739), ('elephant', 0.7022231221199036)]\n",
      "[('jet', 0.5967898368835449), ('flies', 0.5891650319099426), ('drive', 0.5867391228675842), ('pull', 0.5818897485733032), ('parachute', 0.578174889087677)]\n",
      "[('config', 0.7629120945930481), ('compiler', 0.761841893196106), ('configuration', 0.7574464082717896), ('debug', 0.7505757212638855), ('debugger', 0.7458028793334961)]\n",
      "[('reservoir', 0.7019591927528381), ('wetland', 0.695947527885437), ('grassland', 0.6928675174713135), ('woodland', 0.6845004558563232), ('forest,', 0.6844643354415894)]\n",
      "[('software', 0.7691904902458191), ('apps', 0.7486153841018677), ('software,', 0.7409051656723022), ('intranet', 0.7288742065429688), ('desktop', 0.7222436666488647)]\n",
      "\n",
      "[('jumps', 0.8442874550819397), ('bounces', 0.7764142751693726), ('zooms', 0.7475295066833496), ('climbs', 0.722419023513794), ('sinks', 0.7054743766784668)]\n",
      "[('explodes', 0.7586313486099243), ('swells', 0.6751590967178345), ('explodes.', 0.6657760739326477), ('exploding', 0.6483156681060791), ('burns', 0.6342098116874695)]\n",
      "[('pants', 0.7970409989356995), ('trousers', 0.778022289276123), ('shorts', 0.7772522568702698), ('socks', 0.7597492337226868), ('jeans', 0.7595232129096985)]\n",
      "[('running', 0.707077145576477), ('run…', 0.6940699815750122), ('ran', 0.6742272973060608), ('jump', 0.6720276474952698), ('go', 0.6582849621772766)]\n",
      "[('think…', 0.7141529321670532), ('assume', 0.6930972337722778), ('know', 0.6692168712615967), ('thin…', 0.6680185794830322), ('hav…', 0.6656061410903931)]\n"
     ]
    }
   ],
   "source": [
    "def anlg(pos1, neg, pos2):\n",
    "    return twitEmbs.most_similar(positive=[pos1, pos2], negative=[neg], topn=5)\n",
    "# not doing all 3 \n",
    "\n",
    "# I like to imagine this as \"Take A. Subtract B. Add C.\"\n",
    "print(anlg('Copenhagen', 'Denmark', 'England')) # Dublin, i guess?\n",
    "print(anlg('he',         'boy',     'girl'   )) # She, perfect.\n",
    "print(anlg('blue',       'sky',     'grass'  )) # MUSTARD????? blue-sky+grass=mustard. (at least green is #2)\n",
    "print(anlg('nicer',      'nice',    'good'   )) # better, perfect.\n",
    "print(anlg('worked',     'work',    'drink'  )) # drank, perfect.\n",
    "print(anlg('Brazillian', 'Brazil',  'Denmark')) # Dutch. Burn twitter to the ground.\n",
    "print()\n",
    "\n",
    "# a) - homonyms? (is this even semantic?)\n",
    "# interested to see what kind of \"alternatives/synonyms\" it tries finding if I try to specify what meaning of a homonym im looking for\n",
    "print(anlg('mouse',   'computer',   'animal'  )) # hedgehog haha. Not very mouse-like animals.\n",
    "print(anlg('fly',     'animal',     'machine' )) # jet, makes sense. I see parachute too down there.\n",
    "print(anlg('command', 'slave',      'linux'   )) # config and lots of nerd terms lol\n",
    "print(anlg('remote',  'television', 'forest'  )) # i guess some of these make sense\n",
    "print(anlg('web',     'spider',     'computer')) # software, I had excpected internet. intrAnet is there...\n",
    "print()\n",
    "\n",
    "# b) - adding s\n",
    "print(anlg('runs', 'run', 'jump')) # jumps, 3rd person singular present tense\n",
    "print(anlg('jumps', 'jump', 'explode')) # explodes\n",
    "print(anlg('shoes', 'shoe', 'pant')) # pants, mixing it up here. Now the s is plural\n",
    "print(anlg('pants', 'pant', 'run')) # running, mixing the two. Unsurprisingly, didn't quite get it but running is close.\n",
    "print(anlg('phones', 'phone', 'think')) # adds ellipses for some reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Learning Word Embeddings\n",
    "\n",
    "\n",
    "So far you've learned about distributional semantics (vector semantics) in both the traditional and modern neural way, and you qualitatively worked with pre-trained (off-the-shelf) word embeddings in the last assignment.\n",
    "\n",
    "In this assignment, you will learn how to implement a neural network  to learn word embeddings, namely the *Continous Bag of Words* (CBOW) model for word embeddings. More specifically, you will:\n",
    "\n",
    "* learn how to represent text for windows language modeling\n",
    "* learn how to design a Pytorch model (`torch.module`)\n",
    "* learn how to implement a FNN for learning embeddings with CBOW which *sums* the context embedding vectors\n",
    "* train the model for a few epochs using stochastic gradient descent (SGD)\n",
    "* read off the learned embeddings $W$, store them in a gensim-readable file and inspect them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "\n",
    "\n",
    "\n",
    "CBOW is a model proposed by [Mikolov et al., 2013](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n",
    "\n",
    "It is a simple neural method to learn word embeddings and it is one of the two core algorithms in the `word2vec` toolkit (see figure below). Note that, besides its usage here to learn word embeddings, CBOW is also a more general term used to refer to any input representation which consists of (some) way of aggregating a set of word embeddings. Hence its name, the continous BOW representation. You can in fact use such a similar representation (e.g., the average of the embeddings of words) for other tasks as well, such as text classification. Here, CBOW is meant in its original formulation: a network over the *sum* of embeddings of context words aimed at predicting the middle target word. It is related in spirit to a language model, but instead framed as a classification task (with context available on both sides) and hence bears more similarities with a *[word close test](https://en.wikipedia.org/wiki/Cloze_test)*.\n",
    "\n",
    "Illustration of the CBOW model (in comparison to the skip-gram):\n",
    "<img src=\"pics/cbow-vs-skipgram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus 6. Representing the data\n",
    "\n",
    "Given a corpus, extract the training data for the CBOW model using a window size of 2 words on each side of the target word. The following image shows what the input of the training algorithm (`Input`) should look like (`Training window`):\n",
    "\n",
    "\n",
    "<img src=\"pics/cbow-window.jpg\">\n",
    "\n",
    "Hints:\n",
    "* Remember to `\"<pad>\"` the input when the window size is smaller than the expected window size. This also means that the `\"<pad>\"` token should be in the vocabulary; reserve the first `0` index for this special token.\n",
    "* In Pytorch, all input is expected to be a `torch.tensor`. You can create these beforehand with `torch.zeros()`, or just convert a resulting python list by using `torch.tensor(train_data)`.\n",
    "\n",
    "Example:\n",
    "\n",
    "Given the following tiny corpus:\n",
    "```\n",
    "tiny_corpus = [\"this is an example\", \"this is a longer example sentence\", \"I love deep learning\"]\n",
    "```\n",
    "\n",
    "To create the `train_X` data, you first need to extract n-gram windows and the target words:\n",
    "\n",
    "```\n",
    "label,context\n",
    "this ['<pad>', '<pad>', 'is', 'an']\n",
    "is ['<pad>', 'this', 'an', 'example']\n",
    "example ['this', 'is', 'example', '<pad>']\n",
    "...\n",
    "```\n",
    "\n",
    "And convert them into numeric format, where each word token is represented by its unique index:\n",
    "\n",
    "```\n",
    "train_labels = [ 1,  2,  3,  4,  1,  2,  5,  6,  4,  7,  8,  9, 10, 11]\n",
    "train_data = [[ 0,  0,  2,  3],\n",
    " [ 0,  1,  3,  4],\n",
    " [ 1,  2,  4,  0],\n",
    " [ 2,  3,  0,  0],\n",
    " [ 0,  0,  2,  5],\n",
    " [ 0,  1,  5,  6],\n",
    " [ 1,  2,  6,  4],\n",
    " [ 2,  5,  4,  7],\n",
    " [ 5,  6,  7,  0],\n",
    " [ 6,  4,  0,  0],\n",
    " [ 0,  0,  9, 10],\n",
    " [ 0,  8, 10, 11],\n",
    " [ 8,  9, 11,  0],\n",
    " [ 9, 10,  0,  0]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_corpus = [\"this is an example\", \"this is a longer example sentence\", \"I love deep learning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggestion: Implement all your steps first on the `tiny_corpus` data. Then test your implementation on the provided data `sample.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, 'this': 1, 'is': 2, 'an': 3, 'example': 4, 'a': 5, 'longer': 6, 'sentence': 7, 'I': 8, 'love': 9, 'deep': 10, 'learning': 11}\n",
      "['<PAD>', 'this', 'is', 'an', 'example', 'a', 'longer', 'sentence', 'I', 'love', 'deep', 'learning']\n",
      "['this', 'is', 'an', 'example', 'this', 'is', 'a', 'longer', 'example', 'sentence', 'I', 'love', 'deep', 'learning']\n",
      "[['<PAD>', 'is'], ['this', 'an'], ['is', 'example'], ['an', '<PAD>'], ['<PAD>', 'is'], ['this', 'a'], ['is', 'longer'], ['a', 'example'], ['longer', 'sentence'], ['example', '<PAD>'], ['<PAD>', 'love'], ['I', 'deep'], ['love', 'learning'], ['deep', '<PAD>']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## global settings\n",
    "PAD = \"<PAD>\"\n",
    "window_size=2\n",
    "\n",
    "### your code here\n",
    "def tokenize(corpus: list[str]): # does this count as tokenization?\n",
    "    words = pd.unique(np.array([item for sublist in [i.split() for i in corpus] for item in sublist])) # holy shit list comprehension\n",
    "    words = np.append(PAD, words)\n",
    "    indices = list(range(len(words)))\n",
    "\n",
    "    wrd2idx = { word: index for word, index in zip(words, indices) }\n",
    "    idx2wrd = words\n",
    "\n",
    "    return wrd2idx, list(idx2wrd)\n",
    "\n",
    "wrd2idx, idx2wrd = tokenize(tiny_corpus)\n",
    "print(wrd2idx)\n",
    "print(idx2wrd)\n",
    "\n",
    "def create_train_data(corpus):\n",
    "    wrd2idx, idx2wrd = tokenize(corpus)\n",
    "    mid = window_size // 2 # no uneven window sizes\n",
    "    labels_idx = []\n",
    "    labels_wrd = []\n",
    "    train_idx = []\n",
    "    train_wrd = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        sentence = sentence.split()\n",
    "        pad = [PAD] * mid\n",
    "        sentence = pad + sentence + pad\n",
    "\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word == PAD:\n",
    "                continue # im sorry\n",
    "\n",
    "            before = sentence[i-mid:i]\n",
    "            after = sentence[i+1:i+mid+1]\n",
    "            x = before + after\n",
    "            xi = [wrd2idx[w] for w in x]\n",
    "            yi = wrd2idx[word]\n",
    "\n",
    "            train_idx.append(xi)\n",
    "            labels_idx.append(yi)\n",
    "            train_wrd.append(x)\n",
    "            labels_wrd.append(word)\n",
    "    return labels_idx, train_idx, labels_wrd, train_wrd\n",
    "labels_idx, train_idx, labels_wrd, train_wrd = create_train_data(tiny_corpus)\n",
    "print(labels_wrd)\n",
    "print(train_wrd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bonus 7. Implement the continuous bag of words model for estimating word embeddings\n",
    "\n",
    "Implement the CBOW model for word embeddings: a CBOW with window size 2, which `sums` the input embeddings and from that hidden representation `predicts` the target token. \n",
    "\n",
    "The steps for CBOW are as follows:\n",
    "* Convert your data to the center/window (done in bonus 6)\n",
    "* The model should have an embedding layer and a linear layer (and optionally a loss function, you can also put the loss function in the forward loop)\n",
    "* In the forward function of the model, it should: look up the embeddings, sum them, convert to logits (in the linear layer), and optionally calculate the loss (can also be done in forward loop)\n",
    "* In the training loop, we have a for loop for the epochs and one for the data. Within this, we call the forward function and obtain the loss after which the backward pass can be called\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model in Pytorch, one has to define a sub-class of `torch.nn.module` (see also train.py). The constructor `__init__()` and the `forward()` function can then be defined to specify the structure of the network. In the `__init__` function, the layers are specified and initialized, whereas the `forward` function defines how the layers interact during a forward-pass. You can use [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) for the embedding layer, [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) for the hidden layer, and [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) as loss function. \n",
    "\n",
    "For some examples we refer to this [tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py) and this [introduction](https://towardsdatascience.com/an-easy-introduction-to-pytorch-for-neural-networks-3ea08516bff2).\n",
    "\n",
    "* a) Implement the CBOW network as described above:\n",
    "\n",
    "**Hint**: you can print the structure of the model by simply printing the initialized variable. Make sure all the layers are represented in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, gold):\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n\u001b[1;32m---> 14\u001b[0m cbow_model \u001b[38;5;241m=\u001b[39m CBOW(embed_dim,\u001b[38;5;28mlen\u001b[39m(\u001b[43mword2idx\u001b[49m))\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(cbow)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word2idx' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "embed_dim = 64\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, emb_dim, vocab_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def forward(self, inputs, gold):\n",
    "        return inputs\n",
    "\n",
    "cbow_model = CBOW(embed_dim,len(word2idx))\n",
    "print(cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) Now implement the training procedure with gradient descent (`learning rate=0.001`). Go through the dataset `10` times, and update the weights after each line (`batch size = 1`). An example of a training procedure can be found on: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#train-the-network\n",
    "\n",
    "**Hint**: you have to convert the lists created in assignment 3 to be able to do the forward pass. The forward pass expects its input to be in tensors. So for the gold labels this means we have to ensure that we do not pass a zero-dimension tensor which looks like: `tensor(1)`, but convert this to `tensor([1])`. Similarly for the training data, we convert `tensor([0, 0, 2, 3])` to `tensor([[0], [0], [2], [3]])`. This can be done with [tensor views](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
