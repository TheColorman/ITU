{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - NLP and Deep Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 9: Sequence Prediction with HMMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the Viterbi algorithm for decoding in sequence tagging. More concretely, we are going to build a POS tagger for English web data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Emissions and transition probabilities\n",
    "\n",
    "In this part of the exercise you are going to prepare the emission and transition probabilities to use in the viterbi algorithm. We are going to focus on the task of Parts-Of-Speech (POS) tagging. You can use the following datareader for the following assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll_file(path):\n",
    "    \"\"\"\n",
    "    read in conll file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[0])\n",
    "            current_tags.append(tok[1])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are going to use the the POS labels as indices in our Viterbi matrix, we need to know all labels beforehand, and they need to have a static order. We also need a special beginning and end label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length train data: 12543\n",
      "Length dev data: 2000\n",
      "Random datapoint:\n",
      "['It', 'is', 'a', 'time', 'to', 'learn', 'what', 'happened', 'and', 'how', 'it', 'may', 'affect', 'the', 'future', '.']\n",
      "['PRON', 'AUX', 'DET', 'NOUN', 'PART', 'VERB', 'PRON', 'VERB', 'CCONJ', 'ADV', 'PRON', 'AUX', 'VERB', 'DET', 'NOUN', 'PUNCT']\n",
      "All labels:\n",
      "['</S>', '<S>', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "source": [
    "train_data = read_conll_file('pos-data/en_ewt-train.conll')\n",
    "dev_data = read_conll_file('pos-data/en_ewt-dev.conll')\n",
    "\n",
    "SMOOTH = 0.1\n",
    "BEG = '<S>'\n",
    "END = '</S>'\n",
    "UNK = '<UNK>'\n",
    "\n",
    "label_set = set([pos_label for sentence in train_data for pos_label in sentence[1]])\n",
    "label_set.add(BEG)\n",
    "label_set.add(END) #? Why aren't we adding <UNK>?\n",
    "# put labels in a list, so that they are guaranteed to have the same order\n",
    "label_list = list(sorted(label_set))\n",
    "\n",
    "print('Length train data: ' + str(len(train_data)))\n",
    "print('Length dev data: ' + str(len(dev_data)))\n",
    "\n",
    "# the data is a list of pairs, containing 1: a list of words 2: a list of POS labels\n",
    "print('Random datapoint:')\n",
    "print(train_data[70][0])\n",
    "print(train_data[70][1])\n",
    "\n",
    "print('All labels:')\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a) calculate the transition probabilities based on the training data, use a special label for the beginning of a sentence (`<S>`) and the end of a sentence (`</S>`). use laplace smoothing with a value of 0.1 to avoid probabilities of 0.0.\n",
    "\n",
    "**Hint**: The transition probability $P(t_i|t_{i-1})$ is the probability that given a tag, $t_{i-1}$, that it will be followed by a tag $t_i$. \n",
    "$$P(t_i|t_{i-1}) = {C(t_{i-1},t_{i}) \\over C(t_{i-1})}$$\n",
    "With smoothing:\n",
    "$$P(t_i|t_{i-1}) = {C(t_{i-1},t_{i}) + \\gamma \\over C(t_{i-1}) + (|t|) * \\gamma} $$\n",
    "\n",
    "Where $(|t|-1) * \\gamma$ is used because we want to add probability mass to all labels.\n",
    "\n",
    "**Hint2**: Every sentence in the data looks like `['DET', 'NOUN', 'VERB']` without any `<S>` or `</S>` tags. So the beginning and end of every data sample needs to be handled differently when counting occurences of transitions, alternatively you can add these tokens to each data sample so they look like `['<S>, 'DET', 'NOUN', 'VERB', </S>]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "{'</S>': {'</S>': 0.1, '<S>': 0.1, 'ADJ': 0.1, 'ADP': 0.1, 'ADV': 0.1, 'AUX': 0.1, 'CCONJ': 0.1, 'DET': 0.1, 'INTJ': 0.1, 'NOUN': 0.1, 'NUM': 0.1, 'PART': 0.1, 'PRON': 0.1, 'PROPN': 0.1, 'PUNCT': 0.1, 'SCONJ': 0.1, 'SYM': 0.1, 'VERB': 0.1, 'X': 0.1}, '<S>': {'</S>': 0.1, '<S>': 0.1, 'ADJ': 512.1, 'ADP': 548.1, 'ADV': 958.1, 'AUX': 362.1, 'CCONJ': 291.1, 'DET': 1260.1, 'INTJ': 405.1, 'NOUN': 783.1, 'NUM': 495.1, 'PART': 59.1, 'PRON': 3535.1, 'PROPN': 1442.1, 'PUNCT': 443.1, 'SCONJ': 446.1, 'SYM': 91.1, 'VERB': 759.1, 'X': 154.1}, 'ADJ': {'</S>': 50.1, '<S>': 0.1, 'ADJ': 714.1, 'ADP': 1006.1, 'ADV': 181.1, 'AUX': 45.1, 'CCONJ': 562.1, 'DET': 66.1, 'INTJ': 4.1, 'NOUN': 6803.1, 'NUM': 109.1, 'PART': 414.1, 'PRON': 171.1, 'PROPN': 872.1, 'PUNCT': 1702.1, 'SCONJ': 300.1, 'SYM': 20.1, 'VERB': 117.1, 'X': 16.1}, 'ADP': {'</S>': 1.1, '<S>': 0.1, 'ADJ': 1267.1, 'ADP': 540.1, 'ADV': 284.1, 'AUX': 15.1, 'CCONJ': 104.1, 'DET': 6377.1, 'INTJ': 5.1, 'NOUN': 2903.1, 'NUM': 717.1, 'PART': 15.1, 'PRON': 2380.1, 'PROPN': 2505.1, 'PUNCT': 431.1, 'SCONJ': 55.1, 'SYM': 58.1, 'VERB': 115.1, 'X': 23.1}, 'ADV': {'</S>': 34.1, '<S>': 0.1, 'ADJ': 1422.1, 'ADP': 923.1, 'ADV': 900.1, 'AUX': 476.1, 'CCONJ': 263.1, 'DET': 462.1, 'INTJ': 10.1, 'NOUN': 143.1, 'NUM': 184.1, 'PART': 178.1, 'PRON': 912.1, 'PROPN': 101.1, 'PUNCT': 1731.1, 'SCONJ': 359.1, 'SYM': 29.1, 'VERB': 1973.1, 'X': 13.1}, 'AUX': {'</S>': 3.1, '<S>': 0.1, 'ADJ': 1367.1, 'ADP': 423.1, 'ADV': 1805.1, 'AUX': 1014.1, 'CCONJ': 25.1, 'DET': 1030.1, 'INTJ': 4.1, 'NOUN': 152.1, 'NUM': 116.1, 'PART': 1519.1, 'PRON': 603.1, 'PROPN': 94.1, 'PUNCT': 213.1, 'SCONJ': 106.1, 'SYM': 10.1, 'VERB': 4341.1, 'X': 3.1}, 'CCONJ': {'</S>': 3.1, '<S>': 0.1, 'ADJ': 586.1, 'ADP': 187.1, 'ADV': 548.1, 'AUX': 380.1, 'CCONJ': 1.1, 'DET': 650.1, 'INTJ': 20.1, 'NOUN': 974.1, 'NUM': 97.1, 'PART': 105.1, 'PRON': 1225.1, 'PROPN': 519.1, 'PUNCT': 87.1, 'SCONJ': 114.1, 'SYM': 26.1, 'VERB': 1162.1, 'X': 4.1}, 'DET': {'</S>': 0.1, '<S>': 0.1, 'ADJ': 3815.1, 'ADP': 187.1, 'ADV': 257.1, 'AUX': 26.1, 'CCONJ': 10.1, 'DET': 161.1, 'INTJ': 0.1, 'NOUN': 9681.1, 'NUM': 250.1, 'PART': 6.1, 'PRON': 85.1, 'PROPN': 1258.1, 'PUNCT': 209.1, 'SCONJ': 2.1, 'SYM': 32.1, 'VERB': 309.1, 'X': 11.1}, 'INTJ': {'</S>': 16.1, '<S>': 0.1, 'ADJ': 1.1, 'ADP': 6.1, 'ADV': 23.1, 'AUX': 15.1, 'CCONJ': 8.1, 'DET': 10.1, 'INTJ': 16.1, 'NOUN': 13.1, 'NUM': 6.1, 'PART': 0.1, 'PRON': 56.1, 'PROPN': 17.1, 'PUNCT': 261.1, 'SCONJ': 3.1, 'SYM': 2.1, 'VERB': 240.1, 'X': 0.1}, 'NOUN': {'</S>': 554.1, '<S>': 0.1, 'ADJ': 391.1, 'ADP': 7196.1, 'ADV': 1224.1, 'AUX': 2578.1, 'CCONJ': 2470.1, 'DET': 254.1, 'INTJ': 20.1, 'NOUN': 4308.1, 'NUM': 197.1, 'PART': 987.1, 'PRON': 1430.1, 'PROPN': 280.1, 'PUNCT': 10066.1, 'SCONJ': 841.1, 'SYM': 108.1, 'VERB': 1854.1, 'X': 53.1}, 'NUM': {'</S>': 97.1, '<S>': 0.1, 'ADJ': 139.1, 'ADP': 328.1, 'ADV': 31.1, 'AUX': 46.1, 'CCONJ': 119.1, 'DET': 18.1, 'INTJ': 1.1, 'NOUN': 1440.1, 'NUM': 348.1, 'PART': 9.1, 'PRON': 40.1, 'PROPN': 164.1, 'PUNCT': 1135.1, 'SCONJ': 18.1, 'SYM': 135.1, 'VERB': 45.1, 'X': 13.1}, 'PART': {'</S>': 2.1, '<S>': 0.1, 'ADJ': 230.1, 'ADP': 45.1, 'ADV': 267.1, 'AUX': 404.1, 'CCONJ': 14.1, 'DET': 84.1, 'INTJ': 0.1, 'NOUN': 442.1, 'NUM': 21.1, 'PART': 31.1, 'PRON': 51.1, 'PROPN': 64.1, 'PUNCT': 111.1, 'SCONJ': 10.1, 'SYM': 0.1, 'VERB': 3972.1, 'X': 0.1}, 'PRON': {'</S>': 12.1, '<S>': 0.1, 'ADJ': 719.1, 'ADP': 715.1, 'ADV': 952.1, 'AUX': 6066.1, 'CCONJ': 211.1, 'DET': 352.1, 'INTJ': 30.1, 'NOUN': 2499.1, 'NUM': 65.1, 'PART': 229.1, 'PRON': 539.1, 'PROPN': 120.1, 'PUNCT': 1043.1, 'SCONJ': 127.1, 'SYM': 18.1, 'VERB': 4936.1, 'X': 9.1}, 'PROPN': {'</S>': 637.1, '<S>': 0.1, 'ADJ': 168.1, 'ADP': 802.1, 'ADV': 212.1, 'AUX': 804.1, 'CCONJ': 772.1, 'DET': 59.1, 'INTJ': 2.1, 'NOUN': 1000.1, 'NUM': 424.1, 'PART': 541.1, 'PRON': 127.1, 'PROPN': 2764.1, 'PUNCT': 3125.1, 'SCONJ': 75.1, 'SYM': 33.1, 'VERB': 776.1, 'X': 4.1}, 'PUNCT': {'</S>': 10791.1, '<S>': 0.1, 'ADJ': 687.1, 'ADP': 644.1, 'ADV': 790.1, 'AUX': 381.1, 'CCONJ': 1401.1, 'DET': 820.1, 'INTJ': 147.1, 'NOUN': 1630.1, 'NUM': 420.1, 'PART': 135.1, 'PRON': 1750.1, 'PROPN': 1291.1, 'PUNCT': 1088.1, 'SCONJ': 396.1, 'SYM': 66.1, 'VERB': 1048.1, 'X': 112.1}, 'SCONJ': {'</S>': 0.1, '<S>': 0.1, 'ADJ': 98.1, 'ADP': 50.1, 'ADV': 116.1, 'AUX': 59.1, 'CCONJ': 12.1, 'DET': 514.1, 'INTJ': 6.1, 'NOUN': 140.1, 'NUM': 30.1, 'PART': 40.1, 'PRON': 1824.1, 'PROPN': 221.1, 'PUNCT': 37.1, 'SCONJ': 88.1, 'SYM': 0.1, 'VERB': 603.1, 'X': 1.1}, 'SYM': {'</S>': 87.1, '<S>': 0.1, 'ADJ': 26.1, 'ADP': 5.1, 'ADV': 3.1, 'AUX': 2.1, 'CCONJ': 25.1, 'DET': 4.1, 'INTJ': 1.1, 'NOUN': 88.1, 'NUM': 358.1, 'PART': 1.1, 'PRON': 8.1, 'PROPN': 26.1, 'PUNCT': 22.1, 'SCONJ': 2.1, 'SYM': 11.1, 'VERB': 14.1, 'X': 0.1}, 'VERB': {'</S>': 38.1, '<S>': 0.1, 'ADJ': 1003.1, 'ADP': 4171.1, 'ADV': 1561.1, 'AUX': 153.1, 'CCONJ': 395.1, 'DET': 4175.1, 'INTJ': 22.1, 'NOUN': 1783.1, 'NUM': 286.1, 'PART': 1478.1, 'PRON': 3896.1, 'PROPN': 579.1, 'PUNCT': 1781.1, 'SCONJ': 895.1, 'SYM': 44.1, 'VERB': 326.1, 'X': 8.1}, 'X': {'</S>': 218.1, '<S>': 0.1, 'ADJ': 7.1, 'ADP': 19.1, 'ADV': 1.1, 'AUX': 2.1, 'CCONJ': 5.1, 'DET': 3.1, 'INTJ': 0.1, 'NOUN': 29.1, 'NUM': 3.1, 'PART': 1.1, 'PRON': 10.1, 'PROPN': 8.1, 'PUNCT': 112.1, 'SCONJ': 2.1, 'SYM': 0.1, 'VERB': 4.1, 'X': 217.1}}\n",
      "6803.1\n",
      "0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Make a dictionary of dictionaries, so that we can easily query for a certain probability.\n",
    "# We add the smoothing value as a starting point, as it has to be added to each count.\n",
    "# Note that a list of lists with POS indices is more efficient (but a bit more cumbersome to implement).\n",
    "transition_counts = {label: {label: SMOOTH for label in label_list} for label in label_list}\n",
    "# The count that a NOUN follows an ADJ (empty now)\n",
    "print(transition_counts['ADJ']['NOUN'])\n",
    "\n",
    "# First obtain the raw counts\n",
    "pairs = []\n",
    "for sentence in train_data:\n",
    "    sentence = [[BEG] + sentence[0] + [END], [BEG] + sentence[1] + [END]]\n",
    "    labels = sentence[1]\n",
    "    pairs += [(labels[i], labels[i+1]) for i in range(len(labels)-1)]\n",
    "\n",
    "for label in label_list:\n",
    "    this_tuples = filter(lambda x: x[0] == label, pairs)\n",
    "    counts = Counter(this_tuples)\n",
    "    for (l_from, l_to), count in counts.items():\n",
    "        transition_counts[l_from][l_to] += count\n",
    "\n",
    "print(transition_counts)\n",
    "\n",
    "print(transition_counts['ADJ']['NOUN']) # should be 6803.1\n",
    "\n",
    "# Now fill the transition matrix, note that the outgoing probability of each label should sum to 1.0\n",
    "\n",
    "transition_probs = {label: {label: 0.0 for label in label_list} for label in label_list}\n",
    "\n",
    "for label in label_list:\n",
    "    total_count = sum([count for count in transition_counts[label].values()])\n",
    "    transition_probs[label] = { oth_label: oth_count / total_count for oth_label, oth_count in transition_counts[label].items()}\n",
    "\n",
    "print(sum(transition_probs['ADJ'].values())) # sums to 1, we are happy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) calculate the emission probabilities based on the training data. Make sure that every POS tag can be assigned to an `<UNK>` token, use laplace smoothing with a value of 0.01 to avoid probabilities of 0.0.\n",
    "\n",
    "**Hint**: The emission probability $P(w_i|t_{i})$ is the probability that given a tag, $t_i$, that it will be associated with a given word $w_i$. The formula below shows counts $C$ needed to calculate the probability.\n",
    "\n",
    "$$P(w_i|t_{i}) = {C(t_{i},w_{i}) \\over C(t_{i})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735.1\n",
      "0.03719746383228232\n",
      "0.9999999999997137\n"
     ]
    }
   ],
   "source": [
    "word_set = {UNK}\n",
    "for sent in train_data:\n",
    "    for word in sent[0]:\n",
    "        word_set.add(word)\n",
    "word_list = list(sorted(word_set))\n",
    "\n",
    "# Fill emission counts\n",
    "pairs = []\n",
    "for sentence in train_data:\n",
    "    words = sentence[0]\n",
    "    labels = sentence[1]\n",
    "    pairs += zip(labels, words)\n",
    "\n",
    "emission_counts = {label: {word: SMOOTH for word in word_list} for label in label_list}\n",
    "\n",
    "for label in label_list[3:]:\n",
    "    this_tuples = filter(lambda x: x[0] == label, pairs)\n",
    "    counts = Counter(this_tuples)\n",
    "\n",
    "    for (label, word), count in counts.items():\n",
    "        emission_counts[label][word] += count\n",
    "\n",
    "print(emission_counts['ADP']['at'])\n",
    "\n",
    "# Convert to probabilities\n",
    "emission_probs = {label: {word: SMOOTH for word in word_list} for label in label_list}\n",
    "\n",
    "for label in label_list:\n",
    "    total_count = sum([count for count in emission_counts[label].values()])\n",
    "    emission_probs[label] = { word: count / total_count for word, count in emission_counts[label].items()}\n",
    "\n",
    "print(emission_probs['ADP']['at'])\n",
    "print(sum(emission_probs['ADP'].values())) # still sum to one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check whether your solutions are correct by estimating the probabilities on the data and check whether the probabilities match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5171926196793345\n",
      "0.01123434129302644\n",
      "0.04082136964025221\n",
      "0.003808756338424345\n",
      "2.9909103515414666e-05\n",
      "0.0005740785225418476\n",
      "4.071478883275515e-06\n"
     ]
    }
   ],
   "source": [
    "print(transition_probs['ADJ']['NOUN']) # 0.5171926196793345\n",
    "print(transition_probs['NOUN']['ADJ']) # 0.01123434129302644\n",
    "print(transition_probs[BEG]['ADJ']) # 0.04082136964025221\n",
    "print(transition_probs['ADJ'][END]) # 0.003808756338424345\n",
    "print(emission_probs['NOUN']['calling'])   # 2.9909103515414666e-05\n",
    "print(emission_probs['VERB']['calling'])  # 0.0005740785225418476\n",
    "print(emission_probs['VERB'][UNK])  # 4.071478883275515e-06\n",
    "# perfet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Viterbi algorithm\n",
    "\n",
    "In the image below we see an example of the calculation of the first 2 positions in a Viterbi decoding:\n",
    "<img width=500px src=\"pics/viterbi.jpg\">\n",
    "\n",
    "* a) Implement Viterbi decoding, use the transition and emission probabilities previously estimated (note that we also provide pre-calculated probabilities in `probs_en.pickle`). You can use the example code shown below as a starting point if you like.\n",
    "\n",
    "**Hint**: The implementation can become simpler if you think about the problem as a 2d matrix that needs to be filled (each position in the list is a node in the viterbi decoding, $v_1(7)$, $v_1(6)$, ...). You can first initialize the matrix with 0.0's, and then fill it from left to right.\n",
    "\n",
    "**Hint2**: You need to combine three probabilities for each possible history, and take the max of all possible histories. You do not need to use negative log probabilities for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRON', 'AUX', 'DET', 'ADV', 'ADJ', 'NOUN', 'PUNCT']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "# You can also load the pre-calculate probabilities:\n",
    "# import pickle\n",
    "# transition_probs, emission_probs = pickle.load(open('probs_en.pickle', 'rb'))\n",
    "import numpy as np\n",
    "def viterbi(sentence):\n",
    "    \"\"\"\n",
    "    sentence: list of strings\n",
    "    \"\"\"\n",
    "    columns = len(sentence)\n",
    "    # You don't need the special tokens in the viterbi decoding so we remove them\n",
    "    labels_list_exc_special = label_list.copy()\n",
    "    labels_list_exc_special.remove(BEG)\n",
    "    labels_list_exc_special.remove(END)\n",
    "\n",
    "    rows = len(labels_list_exc_special)\n",
    "    \n",
    "    # Create the full matrix for scores as well as the backtracking.\n",
    "    # e.g. scores[0][3] should get the probability of the best path of \n",
    "    # the first label and the 4th word in the sentence\n",
    "    # Backtrack contains the index of the best tag for the previous word\n",
    "    # e.g. backtrack[0][3] should get the index of the best tag for the 3rd word \n",
    "    # when backtracking from the first label and 4th word\n",
    "    scores = np.array([[0.0 for _ in range(columns)] for _ in range(rows)])\n",
    "    backtrack = np.array([[0 for _ in range(columns)] for _ in range(rows)])\n",
    "    \n",
    "    # Handle the first token separately, as it only has 2 probabilities (emission, transition)\n",
    "    word_position = 0\n",
    "    for pos_tag_idx, pos_tag in enumerate(labels_list_exc_special):\n",
    "        # The probability of the first word given the POS tag:\n",
    "        word = sentence[word_position]\n",
    "        if word not in emission_probs[pos_tag]:\n",
    "            word = UNK\n",
    "        em_prob = emission_probs[pos_tag][word] \n",
    "        \n",
    "        # The probability of the POS tag given that the previous \"tag\" is <S>\n",
    "        transition_prob = transition_probs[BEG][pos_tag]\n",
    "        \n",
    "        # Save the total probability:\n",
    "        scores[pos_tag_idx][word_position] = em_prob * transition_prob\n",
    "        \n",
    "        # Backtracking for the first token is uneccessary so we ignore it\n",
    "\n",
    "    # Now handle the rest of the sequence\n",
    "    for word_position in range(1, columns):\n",
    "        word = sentence[word_position]\n",
    "        if word not in emission_probs[pos_tag]:\n",
    "            word = UNK\n",
    "        for pos_tag_idx, pos_tag in enumerate(labels_list_exc_special):\n",
    "            # Get emission probability, remember to handle unknown words\n",
    "            em_prob = emission_probs[pos_tag][word]\n",
    "            \n",
    "            # For each possible history path (i.e. label): get the total score\n",
    "            # We need to get the transition probability and the history probability\n",
    "            # Hint: the history probability is the score of the previous word position in scores matrix\n",
    "            candidate_scores = [0.0] * len(labels_list_exc_special)\n",
    "            for hist_pos_tag_idx, hist_pos_tag in enumerate(labels_list_exc_special):\n",
    "                score_previous = scores[hist_pos_tag_idx][word_position-1]\n",
    "                transition_to_current_tag = transition_probs[hist_pos_tag][pos_tag]\n",
    "                candidate_scores[hist_pos_tag_idx] = score_previous * transition_to_current_tag\n",
    "\n",
    "            # Now extract the best score from candidate_scores and its previous path and save these\n",
    "            # Hint: backtrack should contain the index of the best tag:\n",
    "            # backtrack[tag_idx][word_position] = previous_best_tag_idx\n",
    "            last_word_best_tag_idx, last_word_best_score = max(enumerate(candidate_scores), key=itemgetter(1))\n",
    "            backtrack[pos_tag_idx][word_position] = last_word_best_tag_idx\n",
    "            scores[pos_tag_idx][word_position] = last_word_best_score * em_prob\n",
    "\n",
    "\n",
    "    # Extract the best score from the last labels to the special end label\n",
    "    # Hint: here you only have history and transition (no emission)\n",
    "    candidate_scores = [0.0] * len(labels_list_exc_special)\n",
    "    for pos_tag_idx, pos_tag in enumerate(labels_list_exc_special):\n",
    "        transition_prob = transition_probs[pos_tag][END]\n",
    "        previous_tag_score = scores[pos_tag_idx][-1]\n",
    "        candidate_scores[pos_tag_idx] = transition_prob * previous_tag_score\n",
    "    last_word_best_tag_idx, last_word_best_score = max(enumerate(candidate_scores), key=itemgetter(1))\n",
    "    end_score = candidate_scores[last_word_best_tag_idx]\n",
    "\n",
    "    # Extract the path from the best last label using the backtrack matrix\n",
    "    # Hint: the path contains the index of the best tag for each word\n",
    "    tags_idx = [last_word_best_tag_idx]\n",
    "    for word_idx in range(columns-1, 0, -1):\n",
    "        nexttag = backtrack[tags_idx[0]][word_idx]\n",
    "        tags_idx.insert(0, nexttag)\n",
    "    \n",
    "    return [labels_list_exc_special[i] for i in tags_idx]\n",
    "\n",
    "    # Reverse the path and convert the indexes to labels\n",
    "viterbi(['this', 'is', 'a', 'very', 'good', 'chocolate', '.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* b) Ensure that the best path is saved during the decoding, so that you can extract the labels. What is the accuracy of your implementation of the Viterbi algorithm on the development data (`pos-data/en_ewt-dev.conll`)?\n",
    "\n",
    "**Hint**: If implemented correctly, it should score at least an accuracy of 50%. If you score lower, we suggest you try printing the probabilities at each step (word) for the first sentence of the development data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.14% accuracy\n"
     ]
    }
   ],
   "source": [
    "corr, tot = 0, 0\n",
    "for words, labels in dev_data:\n",
    "    corr += sum(np.array(viterbi(words)) == np.array(labels))\n",
    "    tot += len(labels)\n",
    "\n",
    "acc = corr / tot\n",
    "print(f\"{acc*100:.2f}% accuracy\")\n",
    "# üî•üî•üî•"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* c) **Bonus**: try to improve your predictions by inspecting common errors, tuning some of the decisions (e.g. smoothing, weighing the three probabilities) you made, or improving the handling of unknown tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Don't have time to implement, but here are some ideas.\n",
    "1. I saw in thesting the above cell that AP, which is PROPN, got marked as NOUN.  \n",
    "   All-caps words are probably organizations, so try to look at how many allcaps  \n",
    "   words are PROPNs and incorporate that.  \n",
    "2. ok i lied i only have 1 idea, i would have to play around with weights  \n",
    "   (hyperparameters optimization?) to get anything else.  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Subword tokenization\n",
    "\n",
    "BERT models are trained to predict tokens that were masked with a special `[mask]` token. In this assignment you will inspect what it has learned, and whether it has certain preferences (i.e. probing). \n",
    "\n",
    "a) Load the multilingual Bert tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokzr = AutoTokenizer.from_pretrained('bert-base-multilingual-cased', use_fast=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilingual BERT was trained on the 100 most frequent languages of Wikipedia. They used smoothing, to correct inbalances in the data. However, their smoothing is relatively conservative, so high-resource languages have a higher impact on the model, and it is unclear how they sampled for training the tokenizer. Compare the tokenizations for two different language types you know; preferably one higher-resource and one lower-resource. If you only know 1 language, or only high-resource languages, try to use a different variety of the language (for example for English, use social media abbreviations or typos, e.g.: c u tmrw). Can you observe any differences in the results? does it match your intuition of separating mostly meaning-carrying subwords?\n",
    "\n",
    "You can use Figure 1 of https://arxiv.org/pdf/1911.02116.pdf or https://en.wikipedia.org/wiki/List_of_Wikipedias to see how large languages are on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'an', 'example', 'input']\n",
      "['„Åì„Çå„ÅØ', 'ÂÖ•', 'Âäõ', '‰æã', '„Åß', '##„Åô']\n",
      "['Dette', 'er', 'et', 'eksempel', 'input']\n",
      "['–≠—Ç–æ', '–ø—Ä–∏–º–µ—Ä', '–≤–≤', '##–æ–¥–∞']\n"
     ]
    }
   ],
   "source": [
    "print(tokzr.tokenize('this is an example input'))\n",
    "print(tokzr.tokenize('„Åì„Çå„ÅØÂÖ•Âäõ‰æã„Åß„Åô'))\n",
    "# ÂÖ•Âäõ means \"input\" and should have been 1 token\n",
    "print(tokzr.tokenize('Dette er et eksempel input'))\n",
    "# I'm surprised every word has a token, is danish really common enough for that?\n",
    "print(tokzr.tokenize('–≠—Ç–æ –ø—Ä–∏–º–µ—Ä –≤–≤–æ–¥–∞'))\n",
    "# Makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Test whether the `bert-base-cased` model can solve the analogy task that we discussed in the word2vec lecture ([slides](https://github.itu.dk/robv/intro-nlp2023/blob/main/slides/07-vector_semantics.pdf), [assignment](https://github.itu.dk/robv/intro-nlp2023/blob/main/assignments/week4/week4.ipynb)), we can do this by masking the target word we are looking for, and let the model predict which words fit best. We can then use a prompt to discover what the language model would guess. For example, we can use the prompt \"man is to king as woman is to [MASK]\". Try at least two syntactic analogies, and two semantic analogies.\n",
    "You can use the following code:\n",
    "\n",
    "(Note that you need 4gb of RAM for this assignment, otherwise you can use the HPC.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['positive', 'simple', 'negative', 'diagnostic', 'successful']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def getTopN(inputSent, model, tokzr, topn=1):\n",
    "    maskId = tokzr.convert_tokens_to_ids(tokzr.mask_token)\n",
    "    tokenIds = tokzr(inputSent).input_ids\n",
    "    if maskId not in tokenIds:\n",
    "        return 'please include ' + tokzr.mask_token + ' in your input'\n",
    "    maskIndex = tokenIds.index(maskId)\n",
    "    logits = model(torch.tensor([tokenIds])).logits\n",
    "    return tokzr.convert_ids_to_tokens(torch.topk(logits, topn, dim=2).indices[0][maskIndex])\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')\n",
    "tokzr = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "getTopN('This is a [MASK] test.', model, tokzr, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man is to king as woman is to ['man', 'king', 'queen', 'slave', 'rule']\n",
      "Man is to doctor as woman is to ['man', 'nurse', 'cook', 'doctor', 'woman']\n",
      "Denmark is to Copenhagen as England is to ['London', 'Paris', 'Copenhagen', 'Liverpool', 'Edinburgh']\n",
      "Computer is to mouse as animal is to ['man', 'fish', 'mouse', 'cat', 'human']\n"
     ]
    }
   ],
   "source": [
    "print('Man is to king as woman is to', getTopN('Man is to king as woman is to [MASK].', model, tokzr, 5)) # uncredible\n",
    "print('Man is to doctor as woman is to', getTopN('Man is to doctor as woman is to [MASK].', model, tokzr, 5)) # why are they both man\n",
    "print('Denmark is to Copenhagen as England is to', getTopN('Denmark is to Copenhagen as England is to [MASK].', model, tokzr, 5)) # yep\n",
    "print('Computer is to mouse as animal is to', getTopN('Computer is to mouse as animal is to [MASK].', model, tokzr, 5)) # MAN FISH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Test how robust the language model is, does it have an effect on the results of the word predictions if you include punctuations at the end of the sentence?, what about starting with a capital? and do typos have a large impact?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man is to king as woman is to ['man', 'king', 'queen', 'slave', 'rule']\n",
      "man is to king as woman is to ['king', 'queen', 'man', 'slave', 'rule']\n",
      "Man iz to kng as eoman is to ['man', 'do', 'be', 'die', 'say']\n"
     ]
    }
   ],
   "source": [
    "# I already tested the punctuation above, it did real shit if I didn't include a period at the end (it kept predicting puncts)\n",
    "print('Man is to king as woman is to', getTopN('Man is to king as woman is to [MASK].', model, tokzr, 5))\n",
    "print('man is to king as woman is to', getTopN('man is to king as woman is to [MASK].', model, tokzr, 5)) # def a difference, though it's small.\n",
    "\n",
    "print('Man iz to kng as eoman is to', getTopN('Man iz to kng as eoman is to [MASK].', model, tokzr, 5)) # it sure is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Think of some prompts that test whether the model has any gender biases, you can test this for example by using common gendered names or pronouns, swapping them and then check whether the predicted word changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best gender is ['unknown', 'chosen', 'known', 'found', 'female'] by far.\n",
      "Fire's are put out by fire- ['fighters', 'engines', 'fighting', 'ants', 'trucks']\n",
      "['They', 'People', 'Women', 'they', 'We'] tend to earn less.\n",
      "['You', 'I', 'He', 'She', 'We'] should earn less.\n",
      "['His', 'Her', 'My', 'his', 'The'] name is Bob.\n",
      "['Her', 'His', 'My', 'her', 'The'] name is Amanda.\n"
     ]
    }
   ],
   "source": [
    "# I feel like i already did that but sure\n",
    "print('The best gender is', getTopN('The best gender is [MASK] by far.', model, tokzr, 5), 'by far.') # wow female, slay\n",
    "print('Fire\\'s are put out by fire-', getTopN('Fire\\'s are put out by fire-[MASK].', model, tokzr, 5)) # no firemen\n",
    "print(getTopN('[MASK] tend to earn less.', model, tokzr, 5), 'tend to earn less.') # Women is in there\n",
    "print(getTopN('[MASK] should earn less.', model, tokzr, 5), 'should earn less.') # He and She is there.\n",
    "print(getTopN('[MASK] name is Bob.', model, tokzr, 5), 'name is Bob.') # Her is #2? crazy\n",
    "print(getTopN('[MASK] name is Amanda.', model, tokzr, 5), 'name is Amanda.') # His #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Finetune a BERT model\n",
    "\n",
    "We have provided code for training a BERT based classifier, which can be found in `week5/bert/bert-topic.py`. The implementation uses huggingface's transformers library (https://github.com/huggingface/transformers), and simply adds a linear layer to convert the output of the CLS token from the last layer of the masked language model to a label. \n",
    "\n",
    "a) Inspect the code; what should the shape of the output_scores be at the end of the forward pass?, What does this output represent?\n",
    "\n",
    "b) Train the model on your own machine or on the HPC without a GPU (Note that this code needs ~8gb ram), how long does it take?\n",
    "\n",
    "c) Now change the number of maximum training sentences (MAX_TRAIN_SENTS) to 500 and the batch size (BATCH_SIZE) to 32. Note that it will now take very long to train without a GPU. Train the model on the HPC, make sure you reserve a GPU to speed up the training. For more information, see http://hpc.itu.dk/scheduling/templates/gpu/ (only available on ITU network/VPN). Note that the code detects automatically whether a GPU is available. Also note that the transformers library is already installed, and can be loaded with:\n",
    "\n",
    "```\n",
    "module load PyTorch/1.7.1-foss-2020b\n",
    "module load Transformers/4.2.1-foss-2020a-Python-3.8.2\n",
    "``` \n",
    "\n",
    "(which you also have to put in the job script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll have to take the trust-me-bro guarantee that I actually did that and didn't just hand in the assignment that's already 5 days late"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
